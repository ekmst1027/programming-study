{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### python 3.5 version\n",
    "### virtualEnv_name is helloTF\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설치 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Create a constant op\n",
    "# This op is added as a node to the default graph\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "# seart a TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# run the op and get result\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 더하기 예제\n",
    "# Build graph (tensor) using TensorFlow operations\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32) node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1: {} node2: {}\".format(node1,node2))\n",
    "print(\"node3: {}\".format(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2): \n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "# feed data and run graph (operation) sess.run(op)\n",
    "sess = tf.Session() \n",
    "print(\"sess.run(node1, node2): \".format(sess.run([node1, node2])))\n",
    "print(\"sess.run(node3): \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# placeholder(상수가 아닌 변수를 만들어줌)\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b # + provides a shortcut for tf.add(a,b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 Linear Reression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lab2\n",
    "### Linear Regression\n",
    "\n",
    "## 1. Build graph using TF operations\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# tf.Variable의 의미는 tesnsorflow가 사용하는 변수라는 의미 or trainable variable\n",
    "w = tf.Variable(tf.random_normal([1]), name='weight') # 1차원 array에 대한 랜덤값을 넣어줌\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "# Our hypothesis Xw+b\n",
    "hypothesis = x_train * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss funtion\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train)) # 평균을 내줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize(GradientDescent)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.0750494 [-0.23794141] [1.4495381]\n",
      "20 0.37889063 [0.25166675] [1.5712017]\n",
      "40 0.3304627 [0.32791612] [1.5154322]\n",
      "60 0.30000782 [0.36341318] [1.4459333]\n",
      "80 0.2724708 [0.39370286] [1.3781438]\n",
      "100 0.24746223 [0.42223215] [1.3133916]\n",
      "120 0.22474916 [0.44938847] [1.2516685]\n",
      "140 0.20412077 [0.47526553] [1.1928447]\n",
      "160 0.18538578 [0.49992615] [1.1367854]\n",
      "180 0.16837032 [0.5234277] [1.0833608]\n",
      "200 0.15291668 [0.5458249] [1.0324469]\n",
      "220 0.1388814 [0.5671694] [0.98392576]\n",
      "240 0.12613429 [0.5875108] [0.93768483]\n",
      "260 0.114557184 [0.60689634] [0.89361703]\n",
      "280 0.10404264 [0.62537074] [0.85162026]\n",
      "300 0.09449318 [0.64297706] [0.81159717]\n",
      "320 0.0858202 [0.65975577] [0.77345514]\n",
      "340 0.07794332 [0.67574596] [0.7371056]\n",
      "360 0.07078938 [0.69098467] [0.70246434]\n",
      "380 0.06429204 [0.7055072] [0.6694512]\n",
      "400 0.058391064 [0.71934736] [0.63798946]\n",
      "420 0.05303168 [0.7325369] [0.60800636]\n",
      "440 0.048164222 [0.7451068] [0.57943225]\n",
      "460 0.04374354 [0.75708586] [0.5522011]\n",
      "480 0.039728567 [0.7685019] [0.52624965]\n",
      "500 0.036082145 [0.77938145] [0.5015178]\n",
      "520 0.032770358 [0.78974974] [0.47794828]\n",
      "540 0.02976256 [0.7996307] [0.45548654]\n",
      "560 0.027030839 [0.8090473] [0.43408036]\n",
      "580 0.024549866 [0.8180213] [0.4136803]\n",
      "600 0.022296572 [0.8265736] [0.39423886]\n",
      "620 0.020250106 [0.83472407] [0.37571105]\n",
      "640 0.01839147 [0.84249145] [0.358054]\n",
      "660 0.016703427 [0.8498938] [0.34122667]\n",
      "680 0.015170318 [0.85694814] [0.32519034]\n",
      "700 0.013777934 [0.8636712] [0.30990753]\n",
      "720 0.012513325 [0.87007815] [0.29534298]\n",
      "740 0.011364799 [0.876184] [0.2814629]\n",
      "760 0.010321681 [0.8820029] [0.26823515]\n",
      "780 0.009374325 [0.8875482] [0.2556291]\n",
      "800 0.008513925 [0.8928331] [0.24361548]\n",
      "820 0.007732473 [0.8978695] [0.2321665]\n",
      "840 0.007022763 [0.9026693] [0.22125556]\n",
      "860 0.0063781836 [0.9072435] [0.21085736]\n",
      "880 0.005792763 [0.91160274] [0.20094779]\n",
      "900 0.005261078 [0.9157571] [0.19150399]\n",
      "920 0.004778199 [0.9197162] [0.18250401]\n",
      "940 0.004339636 [0.9234893] [0.17392696]\n",
      "960 0.003941323 [0.927085] [0.16575299]\n",
      "980 0.0035795681 [0.9305118] [0.15796319]\n",
      "1000 0.0032510248 [0.9337774] [0.15053949]\n",
      "1020 0.0029526346 [0.93688965] [0.14346468]\n",
      "1040 0.0026816225 [0.93985564] [0.13672233]\n",
      "1060 0.0024354963 [0.94268215] [0.13029689]\n",
      "1080 0.0022119563 [0.9453759] [0.12417337]\n",
      "1100 0.0020089343 [0.9479431] [0.11833768]\n",
      "1120 0.0018245438 [0.95038956] [0.11277623]\n",
      "1140 0.0016570805 [0.95272106] [0.10747615]\n",
      "1160 0.0015049871 [0.954943] [0.10242517]\n",
      "1180 0.0013668495 [0.95706046] [0.09761158]\n",
      "1200 0.0012413982 [0.95907843] [0.09302423]\n",
      "1220 0.0011274588 [0.9610017] [0.0886524]\n",
      "1240 0.0010239761 [0.9628344] [0.08448607]\n",
      "1260 0.0009299933 [0.964581] [0.08051556]\n",
      "1280 0.00084463414 [0.96624565] [0.07673164]\n",
      "1300 0.00076710945 [0.96783197] [0.07312551]\n",
      "1320 0.00069670327 [0.96934366] [0.06968891]\n",
      "1340 0.0006327538 [0.9707845] [0.06641378]\n",
      "1360 0.00057467824 [0.97215754] [0.06329253]\n",
      "1380 0.0005219294 [0.97346604] [0.060318]\n",
      "1400 0.00047402762 [0.974713] [0.05748326]\n",
      "1420 0.00043051786 [0.9759014] [0.05478175]\n",
      "1440 0.00039100146 [0.9770339] [0.05220721]\n",
      "1460 0.00035511664 [0.97811323] [0.04975369]\n",
      "1480 0.00032252312 [0.9791417] [0.04741548]\n",
      "1500 0.00029292112 [0.9801221] [0.04518723]\n",
      "1520 0.00026603419 [0.9810563] [0.04306358]\n",
      "1540 0.00024161609 [0.9819466] [0.04103973]\n",
      "1560 0.00021944154 [0.982795] [0.03911101]\n",
      "1580 0.00019929859 [0.9836036] [0.03727292]\n",
      "1600 0.00018100592 [0.9843741] [0.03552125]\n",
      "1620 0.00016439264 [0.9851085] [0.0338519]\n",
      "1640 0.00014930572 [0.9858084] [0.03226096]\n",
      "1660 0.00013560115 [0.9864752] [0.03074483]\n",
      "1680 0.00012315564 [0.98711085] [0.02929996]\n",
      "1700 0.00011185216 [0.9877167] [0.02792298]\n",
      "1720 0.00010158625 [0.9882939] [0.02661071]\n",
      "1740 9.2262395e-05 [0.988844] [0.02536013]\n",
      "1760 8.37941e-05 [0.98936826] [0.02416833]\n",
      "1780 7.6102944e-05 [0.98986804] [0.02303248]\n",
      "1800 6.911702e-05 [0.9903442] [0.02194995]\n",
      "1820 6.27722e-05 [0.99079806] [0.02091834]\n",
      "1840 5.7011774e-05 [0.9912305] [0.01993523]\n",
      "1860 5.1778377e-05 [0.99164265] [0.01899833]\n",
      "1880 4.7026235e-05 [0.9920354] [0.01810547]\n",
      "1900 4.270986e-05 [0.9924097] [0.01725458]\n",
      "1920 3.8790015e-05 [0.9927664] [0.01644368]\n",
      "1940 3.5229066e-05 [0.9931063] [0.0156709]\n",
      "1960 3.1996187e-05 [0.9934303] [0.01493444]\n",
      "1980 2.905931e-05 [0.99373907] [0.01423258]\n",
      "2000 2.6392636e-05 [0.9940333] [0.01356369]\n"
     ]
    }
   ],
   "source": [
    "## 2. Run/update graph and get results\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph - 사용하기 전에 변수 초기화가 필요함.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(w), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32.047432 [0.10278696] [-0.35075897]\n",
      "20 0.2001765 [1.2835709] [0.05503527]\n",
      "40 0.17424868 [1.2700676] [0.12487458]\n",
      "60 0.15217271 [1.2524035] [0.18874261]\n",
      "80 0.13289353 [1.2358736] [0.24842156]\n",
      "100 0.11605692 [1.220426] [0.3041921]\n",
      "120 0.10135343 [1.2059901] [0.3563102]\n",
      "140 0.08851267 [1.1924995] [0.40501508]\n",
      "160 0.07729875 [1.1798927] [0.45053026]\n",
      "180 0.06750555 [1.1681114] [0.49306452]\n",
      "200 0.058953166 [1.1571016] [0.53281313]\n",
      "220 0.05148421 [1.1468129] [0.5699588]\n",
      "240 0.044961505 [1.1371981] [0.60467166]\n",
      "260 0.039265234 [1.1282128] [0.63711107]\n",
      "280 0.03429062 [1.119816] [0.66742605]\n",
      "300 0.029946256 [1.1119691] [0.6957557]\n",
      "320 0.026152244 [1.1046362] [0.72222996]\n",
      "340 0.022838956 [1.0977834] [0.74697053]\n",
      "360 0.019945439 [1.0913796] [0.77009076]\n",
      "380 0.017418511 [1.0853951] [0.7916966]\n",
      "400 0.015211733 [1.0798024] [0.8118876]\n",
      "420 0.013284514 [1.0745763] [0.8307563]\n",
      "440 0.011601426 [1.069692] [0.8483894]\n",
      "460 0.010131619 [1.065128] [0.8648676]\n",
      "480 0.008848028 [1.0608625] [0.88026655]\n",
      "500 0.007727047 [1.0568765] [0.8946572]\n",
      "520 0.0067480803 [1.0531517] [0.90810525]\n",
      "540 0.0058931382 [1.0496708] [0.9206726]\n",
      "560 0.0051465523 [1.0464178] [0.9324168]\n",
      "580 0.0044945045 [1.0433779] [0.943392]\n",
      "600 0.0039250893 [1.040537] [0.9536483]\n",
      "620 0.0034277956 [1.0378822] [0.9632332]\n",
      "640 0.0029935162 [1.0354012] [0.9721902]\n",
      "660 0.0026142611 [1.0330827] [0.98056054]\n",
      "680 0.0022830714 [1.0309161] [0.98838264]\n",
      "700 0.0019938108 [1.0288914] [0.9956926]\n",
      "720 0.0017412096 [1.0269994] [1.0025238]\n",
      "740 0.0015206125 [1.0252311] [1.0089076]\n",
      "760 0.0013279592 [1.0235786] [1.0148734]\n",
      "780 0.0011597126 [1.0220345] [1.0204486]\n",
      "800 0.001012786 [1.0205915] [1.0256584]\n",
      "820 0.00088447693 [1.0192428] [1.030527]\n",
      "840 0.00077241746 [1.0179826] [1.035077]\n",
      "860 0.0006745589 [1.016805] [1.0393287]\n",
      "880 0.0005890945 [1.0157044] [1.043302]\n",
      "900 0.0005144665 [1.014676] [1.0470152]\n",
      "920 0.000449278 [1.0137147] [1.0504855]\n",
      "940 0.00039236218 [1.0128165] [1.0537282]\n",
      "960 0.00034265214 [1.0119772] [1.0567586]\n",
      "980 0.000299234 [1.0111926] [1.0595909]\n",
      "1000 0.00026132405 [1.0104597] [1.0622373]\n",
      "1020 0.00022821446 [1.0097746] [1.0647104]\n",
      "1040 0.00019930268 [1.0091344] [1.0670216]\n",
      "1060 0.00017405105 [1.0085362] [1.0691812]\n",
      "1080 0.0001520012 [1.0079772] [1.0711995]\n",
      "1100 0.00013274542 [1.0074548] [1.0730858]\n",
      "1120 0.00011592744 [1.0069666] [1.0748483]\n",
      "1140 0.0001012387 [1.0065103] [1.0764955]\n",
      "1160 8.841357e-05 [1.006084] [1.0780348]\n",
      "1180 7.721469e-05 [1.0056856] [1.0794731]\n",
      "1200 6.74338e-05 [1.0053134] [1.0808173]\n",
      "1220 5.888819e-05 [1.0049653] [1.0820736]\n",
      "1240 5.142762e-05 [1.0046401] [1.0832477]\n",
      "1260 4.491188e-05 [1.0043362] [1.084345]\n",
      "1280 3.9221297e-05 [1.0040522] [1.0853702]\n",
      "1300 3.4251632e-05 [1.0037868] [1.0863283]\n",
      "1320 2.991356e-05 [1.0035388] [1.0872238]\n",
      "1340 2.6123502e-05 [1.0033071] [1.0880605]\n",
      "1360 2.2813316e-05 [1.0030905] [1.0888424]\n",
      "1380 1.9922416e-05 [1.0028881] [1.0895731]\n",
      "1400 1.7397551e-05 [1.0026988] [1.0902562]\n",
      "1420 1.5193211e-05 [1.0025221] [1.0908943]\n",
      "1440 1.3269336e-05 [1.0023569] [1.0914907]\n",
      "1460 1.1587386e-05 [1.0022025] [1.092048]\n",
      "1480 1.0119604e-05 [1.0020583] [1.0925688]\n",
      "1500 8.837901e-06 [1.0019234] [1.0930555]\n",
      "1520 7.717788e-06 [1.0017976] [1.0935103]\n",
      "1540 6.740515e-06 [1.0016799] [1.0939353]\n",
      "1560 5.8864725e-06 [1.0015699] [1.0943323]\n",
      "1580 5.1405996e-06 [1.001467] [1.0947036]\n",
      "1600 4.4892777e-06 [1.001371] [1.0950505]\n",
      "1620 3.921089e-06 [1.0012811] [1.0953743]\n",
      "1640 3.424261e-06 [1.0011973] [1.0956773]\n",
      "1660 2.9907392e-06 [1.001119] [1.0959603]\n",
      "1680 2.6116966e-06 [1.0010457] [1.0962247]\n",
      "1700 2.280975e-06 [1.0009773] [1.0964718]\n",
      "1720 1.9921918e-06 [1.0009134] [1.0967027]\n",
      "1740 1.7399652e-06 [1.0008535] [1.0969186]\n",
      "1760 1.5196363e-06 [1.0007976] [1.0971203]\n",
      "1780 1.327117e-06 [1.0007455] [1.0973089]\n",
      "1800 1.1590482e-06 [1.0006967] [1.0974852]\n",
      "1820 1.0122633e-06 [1.0006509] [1.0976498]\n",
      "1840 8.837471e-07 [1.0006084] [1.0978036]\n",
      "1860 7.7191453e-07 [1.0005686] [1.0979474]\n",
      "1880 6.7427567e-07 [1.0005314] [1.0980818]\n",
      "1900 5.8885666e-07 [1.0004966] [1.0982074]\n",
      "1920 5.1433796e-07 [1.0004641] [1.0983247]\n",
      "1940 4.4928782e-07 [1.0004337] [1.0984343]\n",
      "1960 3.923601e-07 [1.0004053] [1.0985367]\n",
      "1980 3.4266083e-07 [1.0003787] [1.0986325]\n",
      "2000 2.992615e-07 [1.000354] [1.098722]\n",
      "테스트 결과는  [6.100492]\n",
      "테스트 결과는  [3.599607]\n",
      "테스트 결과는  [2.5992532 4.5999613]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use x and y in place of x_data and y_data\n",
    "# # placeholders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/336693740/\n",
    "w = tf.Variable(tf.random_normal([1]), name='weight') # 1차원 array에 대한 랜덤값을 넣어줌\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "x = tf.placeholder(tf.float32, shape=[None])\n",
    "y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# Our hypothesis xw+b\n",
    "hypothesis = x * w + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y)) # 평균을 내줌\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# fit the line with new training data\n",
    "for step in range(2001):\n",
    "    cost_val, w_val, b_val, _ = sess.run([cost, w , b, train],\n",
    "                                        feed_dict={x: [1,2,3,4,5], y:[2.1,3.1,4.1,5.1,6.1]}) # _ 변수는 필요없다는 의미\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, w_val, b_val)\n",
    "        \n",
    "# Testing our model\n",
    "print(\"테스트 결과는 \",sess.run(hypothesis, feed_dict={x:[5]}))\n",
    "print(\"테스트 결과는 \",sess.run(hypothesis, feed_dict={x:[2.5]}))\n",
    "print(\"테스트 결과는 \",sess.run(hypothesis, feed_dict={x:[1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 Cost function minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### lab3\n",
    "## 경사하강법이 잘 적용되는지 확인\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=[1,2,3]\n",
    "y=[1,2,3]\n",
    "\n",
    "w = tf.placeholder(tf.float32)\n",
    "# Our hypothesis f or linear model X*W\n",
    "hypothesis = x*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializess global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables for plotting cost function\n",
    "w_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_w = i * 0.1\n",
    "    curr_cost, curr_w = sess.run([cost, w], feed_dict={w: feed_w})\n",
    "    w_val.append(curr_w)\n",
    "    cost_val.append(curr_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the cost function\n",
    "plt.plot(w_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 최소화 모델\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our hypothesis for linear model X*W\n",
    "hypothesis = x*w\n",
    "# cost/loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize : Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((w*x - y) * x)\n",
    "descent = w - learning_rate * gradient\n",
    "update = w.assign(descent)\n",
    "## 위의 4단계는 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)// train=optimizer.minimize(cost) 과 같은 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18.683338 [-0.1552161]\n",
      "1 5.314372 [0.38388473]\n",
      "2 1.5116439 [0.6714052]\n",
      "3 0.42997846 [0.82474947]\n",
      "4 0.122304924 [0.90653306]\n",
      "5 0.034788966 [0.95015097]\n",
      "6 0.009895565 [0.9734138]\n",
      "7 0.0028147365 [0.9858207]\n",
      "8 0.00080063846 [0.9924377]\n",
      "9 0.0002277376 [0.9959668]\n",
      "10 6.477596e-05 [0.997849]\n",
      "11 1.842488e-05 [0.9988528]\n",
      "12 5.2411224e-06 [0.99938816]\n",
      "13 1.4908106e-06 [0.99967366]\n",
      "14 4.24086e-07 [0.99982595]\n",
      "15 1.2061047e-07 [0.9999072]\n",
      "16 3.422913e-08 [0.9999505]\n",
      "17 9.7515915e-09 [0.9999736]\n",
      "18 2.7702072e-09 [0.99998593]\n",
      "19 7.8428286e-10 [0.9999925]\n",
      "20 2.218492e-10 [0.999996]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={x:x_data, y:y_data})\n",
    "    print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}), sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test\n",
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "# set wrong model weights\n",
    "w = tf.Variable(5.0)\n",
    "# Linear model\n",
    "hypothesis = x*w\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize : Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0\n",
      "1 1.2666664\n",
      "2 1.0177778\n",
      "3 1.0011852\n",
      "4 1.000079\n",
      "5 1.0000052\n",
      "6 1.0000004\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "17 1.0\n",
      "18 1.0\n",
      "19 1.0\n",
      "20 1.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 1.0\n",
      "25 1.0\n",
      "26 1.0\n",
      "27 1.0\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 1.0\n",
      "39 1.0\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 1.0\n",
      "49 1.0\n",
      "50 1.0\n",
      "51 1.0\n",
      "52 1.0\n",
      "53 1.0\n",
      "54 1.0\n",
      "55 1.0\n",
      "56 1.0\n",
      "57 1.0\n",
      "58 1.0\n",
      "59 1.0\n",
      "60 1.0\n",
      "61 1.0\n",
      "62 1.0\n",
      "63 1.0\n",
      "64 1.0\n",
      "65 1.0\n",
      "66 1.0\n",
      "67 1.0\n",
      "68 1.0\n",
      "69 1.0\n",
      "70 1.0\n",
      "71 1.0\n",
      "72 1.0\n",
      "73 1.0\n",
      "74 1.0\n",
      "75 1.0\n",
      "76 1.0\n",
      "77 1.0\n",
      "78 1.0\n",
      "79 1.0\n",
      "80 1.0\n",
      "81 1.0\n",
      "82 1.0\n",
      "83 1.0\n",
      "84 1.0\n",
      "85 1.0\n",
      "86 1.0\n",
      "87 1.0\n",
      "88 1.0\n",
      "89 1.0\n",
      "90 1.0\n",
      "91 1.0\n",
      "92 1.0\n",
      "93 1.0\n",
      "94 1.0\n",
      "95 1.0\n",
      "96 1.0\n",
      "97 1.0\n",
      "98 1.0\n",
      "99 1.0\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    print(step, sess.run(w))\n",
    "    sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n",
      "1 [33.84889, 4.6266665, [(33.84889, 4.6266665)]]\n",
      "2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]\n",
      "3 [27.825287, 3.9812808, [(27.825287, 3.9812808)]]\n",
      "4 [25.228262, 3.703028, [(25.228264, 3.703028)]]\n",
      "5 [22.873621, 3.4507453, [(22.873623, 3.4507453)]]\n",
      "6 [20.738752, 3.2220092, [(20.738752, 3.2220092)]]\n",
      "7 [18.803137, 3.0146217, [(18.803137, 3.0146217)]]\n",
      "8 [17.048176, 2.8265903, [(17.048176, 2.8265903)]]\n",
      "9 [15.457013, 2.6561086, [(15.457014, 2.6561086)]]\n",
      "10 [14.014359, 2.5015385, [(14.01436, 2.5015385)]]\n",
      "11 [12.706352, 2.361395, [(12.706352, 2.361395)]]\n",
      "12 [11.520427, 2.2343314, [(11.520427, 2.2343314)]]\n",
      "13 [10.445186, 2.119127, [(10.445186, 2.119127)]]\n",
      "14 [9.470302, 2.0146751, [(9.470302, 2.0146751)]]\n",
      "15 [8.586407, 1.9199722, [(8.586407, 1.9199722)]]\n",
      "16 [7.785009, 1.8341081, [(7.785009, 1.8341081)]]\n",
      "17 [7.0584083, 1.756258, [(7.0584083, 1.756258)]]\n",
      "18 [6.399624, 1.685674, [(6.399624, 1.685674)]]\n",
      "19 [5.8023257, 1.6216778, [(5.8023257, 1.6216778)]]\n",
      "20 [5.260776, 1.5636545, [(5.260776, 1.5636545)]]\n",
      "21 [4.7697697, 1.5110468, [(4.7697697, 1.5110468)]]\n",
      "22 [4.324591, 1.4633491, [(4.324591, 1.4633491)]]\n",
      "23 [3.9209633, 1.4201032, [(3.9209635, 1.4201032)]]\n",
      "24 [3.5550067, 1.3808936, [(3.5550067, 1.3808936)]]\n",
      "25 [3.2232056, 1.3453435, [(3.2232056, 1.3453435)]]\n",
      "26 [2.9223735, 1.3131114, [(2.9223738, 1.3131114)]]\n",
      "27 [2.6496189, 1.2838877, [(2.6496186, 1.2838877)]]\n",
      "28 [2.4023216, 1.2573916, [(2.4023218, 1.2573916)]]\n",
      "29 [2.178105, 1.2333684, [(2.178105, 1.2333684)]]\n",
      "30 [1.9748148, 1.2115873, [(1.9748147, 1.2115873)]]\n",
      "31 [1.7904993, 1.1918392, [(1.7904994, 1.1918392)]]\n",
      "32 [1.623386, 1.1739342, [(1.6233861, 1.1739342)]]\n",
      "33 [1.4718695, 1.1577003, [(1.4718695, 1.1577003)]]\n",
      "34 [1.3344955, 1.1429816, [(1.3344957, 1.1429816)]]\n",
      "35 [1.2099417, 1.1296366, [(1.2099419, 1.1296366)]]\n",
      "36 [1.0970144, 1.1175373, [(1.0970144, 1.1175373)]]\n",
      "37 [0.9946267, 1.1065671, [(0.9946267, 1.1065671)]]\n",
      "38 [0.90179497, 1.0966209, [(0.901795, 1.0966209)]]\n",
      "39 [0.8176275, 1.087603, [(0.81762755, 1.087603)]]\n",
      "40 [0.7413151, 1.0794266, [(0.7413151, 1.0794266)]]\n",
      "41 [0.67212623, 1.0720135, [(0.6721263, 1.0720135)]]\n",
      "42 [0.609394, 1.0652922, [(0.609394, 1.0652922)]]\n",
      "43 [0.5525169, 1.0591983, [(0.5525169, 1.0591983)]]\n",
      "44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]\n",
      "45 [0.45419374, 1.0486636, [(0.45419377, 1.0486636)]]\n",
      "46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]\n",
      "47 [0.37336722, 1.0400037, [(0.37336725, 1.0400037)]]\n",
      "48 [0.33851996, 1.03627, [(0.33852, 1.03627)]]\n",
      "49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]\n",
      "50 [0.27827826, 1.0298156, [(0.2782783, 1.0298156)]]\n",
      "51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]\n",
      "52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]\n",
      "53 [0.20740573, 1.022222, [(0.20740573, 1.022222)]]\n",
      "54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]\n",
      "55 [0.17049654, 1.0182675, [(0.17049655, 1.0182675)]]\n",
      "56 [0.15458433, 1.0165626, [(0.15458435, 1.0165626)]]\n",
      "57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]\n",
      "58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]\n",
      "59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]\n",
      "60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]\n",
      "61 [0.09471202, 1.0101477, [(0.09471202, 1.0101477)]]\n",
      "62 [0.08587202, 1.0092006, [(0.08587202, 1.0092006)]]\n",
      "63 [0.07785805, 1.0083419, [(0.07785805, 1.0083419)]]\n",
      "64 [0.07059129, 1.0075634, [(0.07059129, 1.0075634)]]\n",
      "65 [0.06400236, 1.0068574, [(0.06400236, 1.0068574)]]\n",
      "66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]\n",
      "67 [0.052612226, 1.005637, [(0.052612226, 1.005637)]]\n",
      "68 [0.047702473, 1.005111, [(0.047702473, 1.005111)]]\n",
      "69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]\n",
      "70 [0.03921318, 1.0042014, [(0.03921318, 1.0042014)]]\n",
      "71 [0.035553534, 1.0038093, [(0.035553537, 1.0038093)]]\n",
      "72 [0.032236177, 1.0034539, [(0.03223618, 1.0034539)]]\n",
      "73 [0.029227654, 1.0031315, [(0.029227655, 1.0031315)]]\n",
      "74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]\n",
      "75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]\n",
      "76 [0.021783749, 1.002334, [(0.02178375, 1.002334)]]\n",
      "77 [0.01975123, 1.0021162, [(0.019751232, 1.0021162)]]\n",
      "78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]\n",
      "79 [0.016236702, 1.0017396, [(0.016236704, 1.0017396)]]\n",
      "80 [0.014720838, 1.0015773, [(0.014720838, 1.0015773)]]\n",
      "81 [0.01334699, 1.00143, [(0.013346991, 1.00143)]]\n",
      "82 [0.012100856, 1.0012965, [(0.012100856, 1.0012965)]]\n",
      "83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]\n",
      "84 [0.0099481745, 1.0010659, [(0.009948175, 1.0010659)]]\n",
      "85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]\n",
      "86 [0.008176883, 1.0008761, [(0.008176884, 1.0008761)]]\n",
      "87 [0.007413149, 1.0007943, [(0.007413149, 1.0007943)]]\n",
      "88 [0.006721576, 1.0007201, [(0.006721576, 1.0007201)]]\n",
      "89 [0.0060940585, 1.0006529, [(0.0060940585, 1.0006529)]]\n",
      "90 [0.005525271, 1.000592, [(0.0055252714, 1.000592)]]\n",
      "91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
      "92 [0.004542589, 1.0004867, [(0.004542589, 1.0004867)]]\n",
      "93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
      "94 [0.0037339528, 1.0004001, [(0.003733953, 1.0004001)]]\n",
      "95 [0.0033854644, 1.0003628, [(0.0033854644, 1.0003628)]]\n",
      "96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
      "97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
      "98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
      "99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
     ]
    }
   ],
   "source": [
    "### Optional: compute_gradient and apply_gradient\n",
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "# set wrong model weights\n",
    "w = tf.Variable(5.)\n",
    "# Linear model\n",
    "hypothesis = x*w\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((w*x-y)*x)*2\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost,[w])\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print(step, sess.run([gradient, w, gvs]))\n",
    "    sess.run(apply_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4 Multi-variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_data = [74., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "# Minimize. Need a very small learning rate for this data set\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  13981.487 \n",
      "Prediction:\n",
      " [44.156605 59.6859   55.321766 64.06783  43.903812]\n",
      "10 Cost:  4.4970202 \n",
      "Prediction:\n",
      " [149.12758 185.30627 179.09622 198.85295 139.72308]\n",
      "20 Cost:  4.3691807 \n",
      "Prediction:\n",
      " [149.44286 185.68169 179.46706 199.25552 140.01009]\n",
      "30 Cost:  4.366564 \n",
      "Prediction:\n",
      " [149.4449  185.68224 179.46854 199.25589 140.01114]\n",
      "40 Cost:  4.363952 \n",
      "Prediction:\n",
      " [149.44601 185.68169 179.46892 199.25507 140.01135]\n",
      "50 Cost:  4.361341 \n",
      "Prediction:\n",
      " [149.44713 185.68114 179.4693  199.25424 140.01157]\n",
      "60 Cost:  4.35874 \n",
      "Prediction:\n",
      " [149.44824 185.68059 179.4697  199.25342 140.01176]\n",
      "70 Cost:  4.3561373 \n",
      "Prediction:\n",
      " [149.44933 185.68002 179.47008 199.25258 140.01198]\n",
      "80 Cost:  4.3535366 \n",
      "Prediction:\n",
      " [149.45042 185.67946 179.47044 199.25174 140.01218]\n",
      "90 Cost:  4.350917 \n",
      "Prediction:\n",
      " [149.45152 185.67888 179.47083 199.2509  140.01239]\n",
      "100 Cost:  4.348326 \n",
      "Prediction:\n",
      " [149.45262 185.67833 179.4712  199.25008 140.0126 ]\n",
      "110 Cost:  4.345732 \n",
      "Prediction:\n",
      " [149.45372 185.67776 179.47159 199.24925 140.01282]\n",
      "120 Cost:  4.3431273 \n",
      "Prediction:\n",
      " [149.45482 185.67722 179.47195 199.24841 140.01303]\n",
      "130 Cost:  4.3405266 \n",
      "Prediction:\n",
      " [149.45592 185.67667 179.47234 199.24759 140.01326]\n",
      "140 Cost:  4.337932 \n",
      "Prediction:\n",
      " [149.45702 185.67612 179.47272 199.24675 140.01346]\n",
      "150 Cost:  4.335338 \n",
      "Prediction:\n",
      " [149.4581  185.67555 179.4731  199.24591 140.01367]\n",
      "160 Cost:  4.332736 \n",
      "Prediction:\n",
      " [149.4592  185.675   179.47346 199.24507 140.01389]\n",
      "170 Cost:  4.3301315 \n",
      "Prediction:\n",
      " [149.46028 185.67444 179.47385 199.24423 140.01411]\n",
      "180 Cost:  4.3275523 \n",
      "Prediction:\n",
      " [149.46138 185.67389 179.47421 199.24341 140.01433]\n",
      "190 Cost:  4.3249745 \n",
      "Prediction:\n",
      " [149.46246 185.67336 179.47461 199.24258 140.01456]\n",
      "200 Cost:  4.3223963 \n",
      "Prediction:\n",
      " [149.46356 185.6728  179.47498 199.24176 140.01477]\n",
      "210 Cost:  4.3198037 \n",
      "Prediction:\n",
      " [149.46465 185.67227 179.47536 199.24092 140.015  ]\n",
      "220 Cost:  4.3172193 \n",
      "Prediction:\n",
      " [149.46573 185.6717  179.47572 199.24008 140.01521]\n",
      "230 Cost:  4.314652 \n",
      "Prediction:\n",
      " [149.46681 185.67117 179.47609 199.23926 140.01544]\n",
      "240 Cost:  4.312049 \n",
      "Prediction:\n",
      " [149.4679  185.67061 179.47646 199.2384  140.01566]\n",
      "250 Cost:  4.309475 \n",
      "Prediction:\n",
      " [149.46896 185.67006 179.47682 199.23756 140.01588]\n",
      "260 Cost:  4.3069153 \n",
      "Prediction:\n",
      " [149.47005 185.66954 179.47722 199.23676 140.01613]\n",
      "270 Cost:  4.304339 \n",
      "Prediction:\n",
      " [149.47112 185.66899 179.47757 199.2359  140.01634]\n",
      "280 Cost:  4.301784 \n",
      "Prediction:\n",
      " [149.4722  185.66844 179.47794 199.23508 140.01656]\n",
      "290 Cost:  4.2992134 \n",
      "Prediction:\n",
      " [149.47327 185.6679  179.4783  199.23424 140.01678]\n",
      "300 Cost:  4.296641 \n",
      "Prediction:\n",
      " [149.47433 185.66734 179.47868 199.2334  140.01701]\n",
      "310 Cost:  4.2940764 \n",
      "Prediction:\n",
      " [149.4754  185.66681 179.47905 199.23256 140.01724]\n",
      "320 Cost:  4.291524 \n",
      "Prediction:\n",
      " [149.47647 185.66626 179.4794  199.23172 140.01746]\n",
      "330 Cost:  4.2889686 \n",
      "Prediction:\n",
      " [149.47754 185.66573 179.47977 199.2309  140.0177 ]\n",
      "340 Cost:  4.2864084 \n",
      "Prediction:\n",
      " [149.4786  185.66519 179.48016 199.23007 140.01794]\n",
      "350 Cost:  4.283839 \n",
      "Prediction:\n",
      " [149.47969 185.66467 179.48051 199.22923 140.01817]\n",
      "360 Cost:  4.28131 \n",
      "Prediction:\n",
      " [149.48074 185.66412 179.48088 199.22841 140.0184 ]\n",
      "370 Cost:  4.278739 \n",
      "Prediction:\n",
      " [149.48181 185.66359 179.48125 199.22757 140.01865]\n",
      "380 Cost:  4.276181 \n",
      "Prediction:\n",
      " [149.48288 185.66306 179.48161 199.22673 140.01888]\n",
      "390 Cost:  4.273632 \n",
      "Prediction:\n",
      " [149.48395 185.66252 179.48198 199.2259  140.01912]\n",
      "400 Cost:  4.271091 \n",
      "Prediction:\n",
      " [149.485   185.66199 179.48235 199.22507 140.01935]\n",
      "410 Cost:  4.2685475 \n",
      "Prediction:\n",
      " [149.48607 185.66147 179.48271 199.22424 140.01959]\n",
      "420 Cost:  4.265992 \n",
      "Prediction:\n",
      " [149.48712 185.66092 179.48308 199.2234  140.01984]\n",
      "430 Cost:  4.263454 \n",
      "Prediction:\n",
      " [149.48817 185.66039 179.48344 199.22256 140.02007]\n",
      "440 Cost:  4.2609053 \n",
      "Prediction:\n",
      " [149.48924 185.65988 179.48381 199.22174 140.02032]\n",
      "450 Cost:  4.258357 \n",
      "Prediction:\n",
      " [149.4903  185.65935 179.48418 199.2209  140.02057]\n",
      "460 Cost:  4.2558475 \n",
      "Prediction:\n",
      " [149.49133 185.65881 179.48453 199.22008 140.02081]\n",
      "470 Cost:  4.253298 \n",
      "Prediction:\n",
      " [149.4924  185.65828 179.4849  199.21924 140.02104]\n",
      "480 Cost:  4.25076 \n",
      "Prediction:\n",
      " [149.49345 185.65778 179.48526 199.2184  140.02129]\n",
      "490 Cost:  4.2482266 \n",
      "Prediction:\n",
      " [149.4945  185.65726 179.48563 199.21758 140.02155]\n",
      "500 Cost:  4.24574 \n",
      "Prediction:\n",
      " [149.49553 185.65671 179.48596 199.21674 140.02176]\n",
      "510 Cost:  4.2432094 \n",
      "Prediction:\n",
      " [149.49658 185.6562  179.48634 199.21591 140.02202]\n",
      "520 Cost:  4.240674 \n",
      "Prediction:\n",
      " [149.49763 185.65567 179.48668 199.21507 140.02226]\n",
      "530 Cost:  4.238122 \n",
      "Prediction:\n",
      " [149.49869 185.65515 179.48706 199.21423 140.02252]\n",
      "540 Cost:  4.2356033 \n",
      "Prediction:\n",
      " [149.49973 185.65462 179.4874  199.2134  140.02277]\n",
      "550 Cost:  4.2331214 \n",
      "Prediction:\n",
      " [149.50075 185.6541  179.48775 199.21257 140.02301]\n",
      "560 Cost:  4.230567 \n",
      "Prediction:\n",
      " [149.5018  185.65358 179.48811 199.21172 140.02325]\n",
      "570 Cost:  4.2280574 \n",
      "Prediction:\n",
      " [149.50284 185.65308 179.4885  199.21089 140.02351]\n",
      "580 Cost:  4.2255425 \n",
      "Prediction:\n",
      " [149.50388 185.65254 179.48883 199.21005 140.02376]\n",
      "590 Cost:  4.2230415 \n",
      "Prediction:\n",
      " [149.50493 185.65204 179.4892  199.20924 140.02402]\n",
      "600 Cost:  4.2205143 \n",
      "Prediction:\n",
      " [149.50597 185.65152 179.48956 199.2084  140.02428]\n",
      "610 Cost:  4.2180295 \n",
      "Prediction:\n",
      " [149.50699 185.65102 179.48991 199.20758 140.02454]\n",
      "620 Cost:  4.2155147 \n",
      "Prediction:\n",
      " [149.50803 185.65048 179.49026 199.20674 140.02478]\n",
      "630 Cost:  4.212997 \n",
      "Prediction:\n",
      " [149.50906 185.64998 179.49062 199.2059  140.02504]\n",
      "640 Cost:  4.2104964 \n",
      "Prediction:\n",
      " [149.5101  185.64948 179.49098 199.20508 140.0253 ]\n",
      "650 Cost:  4.2080145 \n",
      "Prediction:\n",
      " [149.51112 185.64897 179.49133 199.20425 140.02556]\n",
      "660 Cost:  4.2054873 \n",
      "Prediction:\n",
      " [149.51216 185.64844 179.49167 199.2034  140.0258 ]\n",
      "670 Cost:  4.203 \n",
      "Prediction:\n",
      " [149.51317 185.64792 179.49202 199.20256 140.02606]\n",
      "680 Cost:  4.2005224 \n",
      "Prediction:\n",
      " [149.5142  185.64742 179.49239 199.20175 140.02632]\n",
      "690 Cost:  4.198024 \n",
      "Prediction:\n",
      " [149.51521 185.64691 179.49272 199.2009  140.02658]\n",
      "700 Cost:  4.1955247 \n",
      "Prediction:\n",
      " [149.51625 185.6464  179.49309 199.20007 140.02684]\n",
      "710 Cost:  4.1930127 \n",
      "Prediction:\n",
      " [149.51727 185.64589 179.49342 199.19922 140.0271 ]\n",
      "720 Cost:  4.190507 \n",
      "Prediction:\n",
      " [149.51831 185.64539 179.49379 199.1984  140.02737]\n",
      "730 Cost:  4.1880274 \n",
      "Prediction:\n",
      " [149.51932 185.64487 179.49413 199.19756 140.02763]\n",
      "740 Cost:  4.1855583 \n",
      "Prediction:\n",
      " [149.52034 185.64438 179.49448 199.19673 140.0279 ]\n",
      "750 Cost:  4.183043 \n",
      "Prediction:\n",
      " [149.52138 185.64388 179.49484 199.1959  140.02818]\n",
      "760 Cost:  4.1805906 \n",
      "Prediction:\n",
      " [149.52238 185.64339 179.4952  199.19508 140.02844]\n",
      "770 Cost:  4.1781154 \n",
      "Prediction:\n",
      " [149.52339 185.64288 179.49554 199.19424 140.0287 ]\n",
      "780 Cost:  4.175614 \n",
      "Prediction:\n",
      " [149.52441 185.64238 179.4959  199.1934  140.02898]\n",
      "790 Cost:  4.173156 \n",
      "Prediction:\n",
      " [149.52542 185.64186 179.49625 199.19258 140.02924]\n",
      "800 Cost:  4.17067 \n",
      "Prediction:\n",
      " [149.52643 185.64136 179.49657 199.19173 140.0295 ]\n",
      "810 Cost:  4.16821 \n",
      "Prediction:\n",
      " [149.52744 185.64087 179.49692 199.1909  140.02977]\n",
      "820 Cost:  4.165713 \n",
      "Prediction:\n",
      " [149.52847 185.64037 179.49728 199.19008 140.03004]\n",
      "830 Cost:  4.163269 \n",
      "Prediction:\n",
      " [149.52948 185.63988 179.49762 199.18925 140.0303 ]\n",
      "840 Cost:  4.1608033 \n",
      "Prediction:\n",
      " [149.53047 185.63937 179.49797 199.18842 140.03058]\n",
      "850 Cost:  4.1583266 \n",
      "Prediction:\n",
      " [149.53148 185.63889 179.49832 199.18758 140.03085]\n",
      "860 Cost:  4.1558537 \n",
      "Prediction:\n",
      " [149.53249 185.63838 179.49864 199.18674 140.03113]\n",
      "870 Cost:  4.153402 \n",
      "Prediction:\n",
      " [149.53348 185.63788 179.499   199.1859  140.03139]\n",
      "880 Cost:  4.1509256 \n",
      "Prediction:\n",
      " [149.5345  185.6374  179.49937 199.18509 140.0317 ]\n",
      "890 Cost:  4.1484675 \n",
      "Prediction:\n",
      " [149.5355  185.63692 179.49971 199.18425 140.03195]\n",
      "900 Cost:  4.1459913 \n",
      "Prediction:\n",
      " [149.53653 185.63643 179.50005 199.18343 140.03224]\n",
      "910 Cost:  4.143531 \n",
      "Prediction:\n",
      " [149.53752 185.63593 179.5004  199.18259 140.03252]\n",
      "920 Cost:  4.1411047 \n",
      "Prediction:\n",
      " [149.5385  185.63542 179.50072 199.18175 140.03278]\n",
      "930 Cost:  4.1386538 \n",
      "Prediction:\n",
      " [149.53949 185.63493 179.50105 199.18091 140.03305]\n",
      "940 Cost:  4.1361995 \n",
      "Prediction:\n",
      " [149.54048 185.63445 179.5014  199.18007 140.03333]\n",
      "950 Cost:  4.133734 \n",
      "Prediction:\n",
      " [149.54149 185.63394 179.50172 199.17923 140.0336 ]\n",
      "960 Cost:  4.1312933 \n",
      "Prediction:\n",
      " [149.54248 185.63347 179.50208 199.1784  140.03389]\n",
      "970 Cost:  4.1288347 \n",
      "Prediction:\n",
      " [149.54349 185.63298 179.50243 199.17758 140.03418]\n",
      "980 Cost:  4.126384 \n",
      "Prediction:\n",
      " [149.54448 185.63249 179.50278 199.17674 140.03445]\n",
      "990 Cost:  4.1239467 \n",
      "Prediction:\n",
      " [149.54549 185.632   179.50313 199.17593 140.03474]\n",
      "1000 Cost:  4.121493 \n",
      "Prediction:\n",
      " [149.54648 185.63153 179.50346 199.1751  140.03503]\n",
      "1010 Cost:  4.1190825 \n",
      "Prediction:\n",
      " [149.54745 185.63104 179.5038  199.17427 140.03531]\n",
      "1020 Cost:  4.116639 \n",
      "Prediction:\n",
      " [149.54845 185.63055 179.50414 199.17343 140.03558]\n",
      "1030 Cost:  4.1142054 \n",
      "Prediction:\n",
      " [149.54941 185.63005 179.50446 199.17258 140.03586]\n",
      "1040 Cost:  4.111755 \n",
      "Prediction:\n",
      " [149.5504  185.62958 179.50479 199.17174 140.03615]\n",
      "1050 Cost:  4.109341 \n",
      "Prediction:\n",
      " [149.55139 185.6291  179.50514 199.17093 140.03644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060 Cost:  4.1069 \n",
      "Prediction:\n",
      " [149.55238 185.62863 179.50548 199.1701  140.03674]\n",
      "1070 Cost:  4.1044903 \n",
      "Prediction:\n",
      " [149.55336 185.62814 179.5058  199.16927 140.037  ]\n",
      "1080 Cost:  4.102021 \n",
      "Prediction:\n",
      " [149.55437 185.62769 179.50616 199.16844 140.03732]\n",
      "1090 Cost:  4.099616 \n",
      "Prediction:\n",
      " [149.55533 185.6272  179.50648 199.1676  140.0376 ]\n",
      "1100 Cost:  4.0971866 \n",
      "Prediction:\n",
      " [149.55632 185.62671 179.50682 199.16678 140.03789]\n",
      "1110 Cost:  4.094754 \n",
      "Prediction:\n",
      " [149.5573  185.62624 179.50717 199.16594 140.03818]\n",
      "1120 Cost:  4.0923395 \n",
      "Prediction:\n",
      " [149.55826 185.62575 179.50749 199.1651  140.03847]\n",
      "1130 Cost:  4.0899234 \n",
      "Prediction:\n",
      " [149.55923 185.62527 179.50783 199.16426 140.03874]\n",
      "1140 Cost:  4.087523 \n",
      "Prediction:\n",
      " [149.56021 185.6248  179.50816 199.16345 140.03905]\n",
      "1150 Cost:  4.085082 \n",
      "Prediction:\n",
      " [149.5612  185.62433 179.5085  199.16261 140.03934]\n",
      "1160 Cost:  4.0826826 \n",
      "Prediction:\n",
      " [149.56216 185.62387 179.50883 199.16179 140.03964]\n",
      "1170 Cost:  4.0802736 \n",
      "Prediction:\n",
      " [149.56314 185.62338 179.50917 199.16096 140.03993]\n",
      "1180 Cost:  4.077852 \n",
      "Prediction:\n",
      " [149.5641  185.62292 179.50949 199.16011 140.04022]\n",
      "1190 Cost:  4.075414 \n",
      "Prediction:\n",
      " [149.56508 185.62245 179.50984 199.15927 140.04053]\n",
      "1200 Cost:  4.0730486 \n",
      "Prediction:\n",
      " [149.56604 185.62198 179.51016 199.15846 140.04082]\n",
      "1210 Cost:  4.0706153 \n",
      "Prediction:\n",
      " [149.56702 185.6215  179.5105  199.15762 140.04112]\n",
      "1220 Cost:  4.0682087 \n",
      "Prediction:\n",
      " [149.56798 185.62102 179.51082 199.15678 140.04141]\n",
      "1230 Cost:  4.065796 \n",
      "Prediction:\n",
      " [149.56894 185.62056 179.51115 199.15594 140.04172]\n",
      "1240 Cost:  4.063419 \n",
      "Prediction:\n",
      " [149.56992 185.6201  179.51149 199.15514 140.042  ]\n",
      "1250 Cost:  4.0610223 \n",
      "Prediction:\n",
      " [149.57086 185.61963 179.51181 199.1543  140.04231]\n",
      "1260 Cost:  4.0586085 \n",
      "Prediction:\n",
      " [149.57184 185.61917 179.51215 199.15346 140.0426 ]\n",
      "1270 Cost:  4.0561943 \n",
      "Prediction:\n",
      " [149.5728  185.61868 179.51247 199.15262 140.04291]\n",
      "1280 Cost:  4.053844 \n",
      "Prediction:\n",
      " [149.57376 185.61824 179.51282 199.15182 140.04321]\n",
      "1290 Cost:  4.051405 \n",
      "Prediction:\n",
      " [149.57474 185.61778 179.51314 199.15097 140.04352]\n",
      "1300 Cost:  4.049025 \n",
      "Prediction:\n",
      " [149.57568 185.61731 179.51349 199.15015 140.04382]\n",
      "1310 Cost:  4.0466385 \n",
      "Prediction:\n",
      " [149.57663 185.61684 179.51378 199.1493  140.04413]\n",
      "1320 Cost:  4.0442557 \n",
      "Prediction:\n",
      " [149.57759 185.6164  179.51411 199.14848 140.04443]\n",
      "1330 Cost:  4.041862 \n",
      "Prediction:\n",
      " [149.57854 185.61592 179.51445 199.14764 140.04474]\n",
      "1340 Cost:  4.03948 \n",
      "Prediction:\n",
      " [149.5795  185.61546 179.51477 199.14682 140.04504]\n",
      "1350 Cost:  4.0370917 \n",
      "Prediction:\n",
      " [149.58046 185.615   179.51509 199.14598 140.04533]\n",
      "1360 Cost:  4.03472 \n",
      "Prediction:\n",
      " [149.5814  185.61453 179.51543 199.14516 140.04564]\n",
      "1370 Cost:  4.032321 \n",
      "Prediction:\n",
      " [149.58237 185.61407 179.51575 199.14432 140.04594]\n",
      "1380 Cost:  4.0299535 \n",
      "Prediction:\n",
      " [149.58331 185.61363 179.51608 199.14351 140.04628]\n",
      "1390 Cost:  4.0275793 \n",
      "Prediction:\n",
      " [149.58426 185.61316 179.5164  199.14267 140.04657]\n",
      "1400 Cost:  4.0251794 \n",
      "Prediction:\n",
      " [149.58522 185.61269 179.51672 199.14183 140.04688]\n",
      "1410 Cost:  4.0228224 \n",
      "Prediction:\n",
      " [149.58617 185.61226 179.51706 199.141   140.04718]\n",
      "1420 Cost:  4.020461 \n",
      "Prediction:\n",
      " [149.5871  185.61179 179.51738 199.14018 140.0475 ]\n",
      "1430 Cost:  4.0180845 \n",
      "Prediction:\n",
      " [149.58804 185.61133 179.51768 199.13934 140.0478 ]\n",
      "1440 Cost:  4.0157037 \n",
      "Prediction:\n",
      " [149.58899 185.61087 179.51802 199.1385  140.04811]\n",
      "1450 Cost:  4.0133452 \n",
      "Prediction:\n",
      " [149.58995 185.61044 179.51834 199.1377  140.04843]\n",
      "1460 Cost:  4.010957 \n",
      "Prediction:\n",
      " [149.5909  185.60999 179.51866 199.13686 140.04875]\n",
      "1470 Cost:  4.00863 \n",
      "Prediction:\n",
      " [149.59181 185.60953 179.51898 199.13603 140.04906]\n",
      "1480 Cost:  4.0062547 \n",
      "Prediction:\n",
      " [149.59276 185.60907 179.5193  199.1352  140.04936]\n",
      "1490 Cost:  4.003888 \n",
      "Prediction:\n",
      " [149.5937  185.60863 179.51964 199.13437 140.04968]\n",
      "1500 Cost:  4.0015364 \n",
      "Prediction:\n",
      " [149.59464 185.60817 179.51996 199.13354 140.05   ]\n",
      "1510 Cost:  3.999155 \n",
      "Prediction:\n",
      " [149.59558 185.60771 179.52026 199.1327  140.05032]\n",
      "1520 Cost:  3.996809 \n",
      "Prediction:\n",
      " [149.59651 185.60727 179.52058 199.13188 140.05064]\n",
      "1530 Cost:  3.994453 \n",
      "Prediction:\n",
      " [149.59744 185.60683 179.52092 199.13104 140.05095]\n",
      "1540 Cost:  3.9921002 \n",
      "Prediction:\n",
      " [149.59837 185.60637 179.52122 199.1302  140.05125]\n",
      "1550 Cost:  3.9897265 \n",
      "Prediction:\n",
      " [149.59932 185.60593 179.52156 199.12938 140.05159]\n",
      "1560 Cost:  3.9873948 \n",
      "Prediction:\n",
      " [149.60025 185.60547 179.52187 199.12856 140.0519 ]\n",
      "1570 Cost:  3.9850335 \n",
      "Prediction:\n",
      " [149.60118 185.60503 179.52219 199.12772 140.05222]\n",
      "1580 Cost:  3.982711 \n",
      "Prediction:\n",
      " [149.60211 185.60458 179.5225  199.1269  140.05254]\n",
      "1590 Cost:  3.9803276 \n",
      "Prediction:\n",
      " [149.60306 185.60414 179.52281 199.12607 140.05287]\n",
      "1600 Cost:  3.9779842 \n",
      "Prediction:\n",
      " [149.60399 185.6037  179.52315 199.12524 140.05319]\n",
      "1610 Cost:  3.9756553 \n",
      "Prediction:\n",
      " [149.6049  185.60326 179.52345 199.1244  140.0535 ]\n",
      "1620 Cost:  3.9733093 \n",
      "Prediction:\n",
      " [149.60582 185.6028  179.52377 199.12357 140.05382]\n",
      "1630 Cost:  3.9709816 \n",
      "Prediction:\n",
      " [149.60674 185.60236 179.52408 199.12273 140.05412]\n",
      "1640 Cost:  3.9686265 \n",
      "Prediction:\n",
      " [149.60768 185.60193 179.52441 199.12192 140.05447]\n",
      "1650 Cost:  3.9663074 \n",
      "Prediction:\n",
      " [149.6086  185.60149 179.52472 199.1211  140.0548 ]\n",
      "1660 Cost:  3.9639688 \n",
      "Prediction:\n",
      " [149.60954 185.60106 179.52504 199.12029 140.05513]\n",
      "1670 Cost:  3.9616294 \n",
      "Prediction:\n",
      " [149.61046 185.60062 179.52536 199.11945 140.05545]\n",
      "1680 Cost:  3.9592915 \n",
      "Prediction:\n",
      " [149.61137 185.60017 179.52568 199.1186  140.05577]\n",
      "1690 Cost:  3.9569702 \n",
      "Prediction:\n",
      " [149.61227 185.59972 179.52597 199.11777 140.05609]\n",
      "1700 Cost:  3.9546554 \n",
      "Prediction:\n",
      " [149.61319 185.59927 179.52628 199.11694 140.05641]\n",
      "1710 Cost:  3.9523022 \n",
      "Prediction:\n",
      " [149.61414 185.59886 179.52661 199.11612 140.05675]\n",
      "1720 Cost:  3.9499862 \n",
      "Prediction:\n",
      " [149.61505 185.59842 179.52693 199.1153  140.05707]\n",
      "1730 Cost:  3.9476364 \n",
      "Prediction:\n",
      " [149.61598 185.59799 179.52725 199.11447 140.05742]\n",
      "1740 Cost:  3.9453323 \n",
      "Prediction:\n",
      " [149.6169  185.59756 179.52757 199.11366 140.05775]\n",
      "1750 Cost:  3.9430232 \n",
      "Prediction:\n",
      " [149.61778 185.5971  179.52786 199.11281 140.05806]\n",
      "1760 Cost:  3.940717 \n",
      "Prediction:\n",
      " [149.6187  185.59668 179.52817 199.11198 140.05838]\n",
      "1770 Cost:  3.9383736 \n",
      "Prediction:\n",
      " [149.61961 185.59624 179.52849 199.11115 140.05872]\n",
      "1780 Cost:  3.93606 \n",
      "Prediction:\n",
      " [149.62053 185.59581 179.52878 199.11032 140.05905]\n",
      "1790 Cost:  3.9337363 \n",
      "Prediction:\n",
      " [149.62143 185.59538 179.5291  199.10948 140.05939]\n",
      "1800 Cost:  3.9314198 \n",
      "Prediction:\n",
      " [149.62236 185.59496 179.52943 199.10867 140.05972]\n",
      "1810 Cost:  3.929105 \n",
      "Prediction:\n",
      " [149.62328 185.59453 179.52974 199.10785 140.06006]\n",
      "1820 Cost:  3.9268093 \n",
      "Prediction:\n",
      " [149.62416 185.59409 179.53004 199.10701 140.06038]\n",
      "1830 Cost:  3.9244964 \n",
      "Prediction:\n",
      " [149.62508 185.59366 179.53035 199.10619 140.06071]\n",
      "1840 Cost:  3.9221988 \n",
      "Prediction:\n",
      " [149.62598 185.59323 179.53065 199.10536 140.06105]\n",
      "1850 Cost:  3.919899 \n",
      "Prediction:\n",
      " [149.62688 185.5928  179.53098 199.10454 140.06139]\n",
      "1860 Cost:  3.917596 \n",
      "Prediction:\n",
      " [149.62779 185.59236 179.53128 199.10371 140.0617 ]\n",
      "1870 Cost:  3.9152856 \n",
      "Prediction:\n",
      " [149.6287  185.59195 179.53159 199.10287 140.06204]\n",
      "1880 Cost:  3.9129853 \n",
      "Prediction:\n",
      " [149.62958 185.5915  179.53188 199.10204 140.06238]\n",
      "1890 Cost:  3.9106965 \n",
      "Prediction:\n",
      " [149.6305  185.5911  179.5322  199.10123 140.06271]\n",
      "1900 Cost:  3.9083886 \n",
      "Prediction:\n",
      " [149.6314  185.59067 179.53252 199.1004  140.06306]\n",
      "1910 Cost:  3.9060624 \n",
      "Prediction:\n",
      " [149.63231 185.59024 179.53282 199.09956 140.0634 ]\n",
      "1920 Cost:  3.9037902 \n",
      "Prediction:\n",
      " [149.63321 185.58983 179.53314 199.09875 140.06374]\n",
      "1930 Cost:  3.901484 \n",
      "Prediction:\n",
      " [149.63411 185.58942 179.53345 199.09792 140.06407]\n",
      "1940 Cost:  3.899203 \n",
      "Prediction:\n",
      " [149.63498 185.58897 179.53374 199.09708 140.0644 ]\n",
      "1950 Cost:  3.896914 \n",
      "Prediction:\n",
      " [149.63588 185.58855 179.53404 199.09625 140.06474]\n",
      "1960 Cost:  3.8946419 \n",
      "Prediction:\n",
      " [149.63676 185.58812 179.53433 199.09543 140.06508]\n",
      "1970 Cost:  3.8923316 \n",
      "Prediction:\n",
      " [149.63768 185.5877  179.53464 199.0946  140.06543]\n",
      "1980 Cost:  3.890052 \n",
      "Prediction:\n",
      " [149.63858 185.5873  179.53496 199.0938  140.06578]\n",
      "1990 Cost:  3.8877544 \n",
      "Prediction:\n",
      " [149.63948 185.58688 179.53528 199.09297 140.06613]\n",
      "2000 Cost:  3.8854897 \n",
      "Prediction:\n",
      " [149.64035 185.58644 179.53557 199.09213 140.06645]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "# 매트릭스 사용 전\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\",hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 매트릭스 사용(리스트 사용)\n",
    "x_data = [[73., 80., 75.], [93., 88., 93.], [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]\n",
    "y_data = [[152.], [185.], [180.], [196.], [142.]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  32446.979 \n",
      "Prediction:\n",
      " [[ -3.4796705]\n",
      " [-10.693423 ]\n",
      " [ -6.9586973]\n",
      " [ -9.330176 ]\n",
      " [ -8.50254  ]]\n",
      "10 Cost:  10.969659 \n",
      "Prediction:\n",
      " [[155.82771]\n",
      " [180.80602]\n",
      " [181.71654]\n",
      " [196.13393]\n",
      " [137.56796]]\n",
      "20 Cost:  10.616253 \n",
      "Prediction:\n",
      " [[156.29715]\n",
      " [181.39403]\n",
      " [182.28355]\n",
      " [196.7529 ]\n",
      " [138.02115]]\n",
      "30 Cost:  10.560113 \n",
      "Prediction:\n",
      " [[156.28604]\n",
      " [181.40439]\n",
      " [182.28142]\n",
      " [196.75209]\n",
      " [138.0337 ]]\n",
      "40 Cost:  10.504267 \n",
      "Prediction:\n",
      " [[156.2735 ]\n",
      " [181.41298]\n",
      " [182.27757]\n",
      " [196.7494 ]\n",
      " [138.04489]]\n",
      "50 Cost:  10.448715 \n",
      "Prediction:\n",
      " [[156.261  ]\n",
      " [181.42154]\n",
      " [182.27373]\n",
      " [196.7467 ]\n",
      " [138.05606]]\n",
      "60 Cost:  10.393481 \n",
      "Prediction:\n",
      " [[156.24854]\n",
      " [181.43008]\n",
      " [182.2699 ]\n",
      " [196.74403]\n",
      " [138.06718]]\n",
      "70 Cost:  10.338527 \n",
      "Prediction:\n",
      " [[156.2361 ]\n",
      " [181.4386 ]\n",
      " [182.26608]\n",
      " [196.74135]\n",
      " [138.0783 ]]\n",
      "80 Cost:  10.283852 \n",
      "Prediction:\n",
      " [[156.22368]\n",
      " [181.44708]\n",
      " [182.26225]\n",
      " [196.73868]\n",
      " [138.08936]]\n",
      "90 Cost:  10.229495 \n",
      "Prediction:\n",
      " [[156.21133]\n",
      " [181.45558]\n",
      " [182.25848]\n",
      " [196.73604]\n",
      " [138.10042]]\n",
      "100 Cost:  10.175464 \n",
      "Prediction:\n",
      " [[156.19899]\n",
      " [181.464  ]\n",
      " [182.25467]\n",
      " [196.73337]\n",
      " [138.1114 ]]\n",
      "110 Cost:  10.121673 \n",
      "Prediction:\n",
      " [[156.18669]\n",
      " [181.47243]\n",
      " [182.25087]\n",
      " [196.73071]\n",
      " [138.12239]]\n",
      "120 Cost:  10.068227 \n",
      "Prediction:\n",
      " [[156.17441]\n",
      " [181.48082]\n",
      " [182.2471 ]\n",
      " [196.72809]\n",
      " [138.13332]]\n",
      "130 Cost:  10.015013 \n",
      "Prediction:\n",
      " [[156.16217]\n",
      " [181.4892 ]\n",
      " [182.24335]\n",
      " [196.72546]\n",
      " [138.14426]]\n",
      "140 Cost:  9.962103 \n",
      "Prediction:\n",
      " [[156.14998]\n",
      " [181.49757]\n",
      " [182.23961]\n",
      " [196.72284]\n",
      " [138.15515]]\n",
      "150 Cost:  9.909469 \n",
      "Prediction:\n",
      " [[156.1378 ]\n",
      " [181.5059 ]\n",
      " [182.23586]\n",
      " [196.72025]\n",
      " [138.16602]]\n",
      "160 Cost:  9.857179 \n",
      "Prediction:\n",
      " [[156.12567]\n",
      " [181.5142 ]\n",
      " [182.23213]\n",
      " [196.7176 ]\n",
      " [138.17683]]\n",
      "170 Cost:  9.805101 \n",
      "Prediction:\n",
      " [[156.11359]\n",
      " [181.52252]\n",
      " [182.22841]\n",
      " [196.71503]\n",
      " [138.18765]]\n",
      "180 Cost:  9.753332 \n",
      "Prediction:\n",
      " [[156.1015 ]\n",
      " [181.53076]\n",
      " [182.22469]\n",
      " [196.71242]\n",
      " [138.19843]]\n",
      "190 Cost:  9.701815 \n",
      "Prediction:\n",
      " [[156.08945]\n",
      " [181.53902]\n",
      " [182.221  ]\n",
      " [196.70984]\n",
      " [138.20917]]\n",
      "200 Cost:  9.650622 \n",
      "Prediction:\n",
      " [[156.07744]\n",
      " [181.54724]\n",
      " [182.2173 ]\n",
      " [196.70726]\n",
      " [138.21986]]\n",
      "210 Cost:  9.59968 \n",
      "Prediction:\n",
      " [[156.06544]\n",
      " [181.55542]\n",
      " [182.21361]\n",
      " [196.70468]\n",
      " [138.23055]]\n",
      "220 Cost:  9.54899 \n",
      "Prediction:\n",
      " [[156.05354]\n",
      " [181.56364]\n",
      " [182.20996]\n",
      " [196.70215]\n",
      " [138.24124]]\n",
      "230 Cost:  9.498631 \n",
      "Prediction:\n",
      " [[156.04161]\n",
      " [181.57176]\n",
      " [182.20628]\n",
      " [196.69955]\n",
      " [138.25185]]\n",
      "240 Cost:  9.44855 \n",
      "Prediction:\n",
      " [[156.02974]\n",
      " [181.5799 ]\n",
      " [182.20264]\n",
      " [196.69702]\n",
      " [138.26244]]\n",
      "250 Cost:  9.398707 \n",
      "Prediction:\n",
      " [[156.0179 ]\n",
      " [181.58801]\n",
      " [182.199  ]\n",
      " [196.69447]\n",
      " [138.27301]]\n",
      "260 Cost:  9.349118 \n",
      "Prediction:\n",
      " [[156.00607]\n",
      " [181.59608]\n",
      " [182.19536]\n",
      " [196.69193]\n",
      " [138.28355]]\n",
      "270 Cost:  9.299795 \n",
      "Prediction:\n",
      " [[155.9943 ]\n",
      " [181.60417]\n",
      " [182.19174]\n",
      " [196.6894 ]\n",
      " [138.29407]]\n",
      "280 Cost:  9.25079 \n",
      "Prediction:\n",
      " [[155.98256]\n",
      " [181.61223]\n",
      " [182.18813]\n",
      " [196.68689]\n",
      " [138.30453]]\n",
      "290 Cost:  9.202005 \n",
      "Prediction:\n",
      " [[155.97084]\n",
      " [181.62024]\n",
      " [182.18454]\n",
      " [196.68437]\n",
      " [138.31502]]\n",
      "300 Cost:  9.153509 \n",
      "Prediction:\n",
      " [[155.95914]\n",
      " [181.62822]\n",
      " [182.18092]\n",
      " [196.68187]\n",
      " [138.32542]]\n",
      "310 Cost:  9.105243 \n",
      "Prediction:\n",
      " [[155.94748]\n",
      " [181.63623]\n",
      " [182.17735]\n",
      " [196.67938]\n",
      " [138.33582]]\n",
      "320 Cost:  9.057255 \n",
      "Prediction:\n",
      " [[155.93585]\n",
      " [181.64418]\n",
      " [182.17377]\n",
      " [196.67685]\n",
      " [138.34618]]\n",
      "330 Cost:  9.009514 \n",
      "Prediction:\n",
      " [[155.92426]\n",
      " [181.65211]\n",
      " [182.1702 ]\n",
      " [196.67436]\n",
      " [138.35652]]\n",
      "340 Cost:  8.962077 \n",
      "Prediction:\n",
      " [[155.91272]\n",
      " [181.66002]\n",
      " [182.16664]\n",
      " [196.67189]\n",
      " [138.36685]]\n",
      "350 Cost:  8.914846 \n",
      "Prediction:\n",
      " [[155.90117]\n",
      " [181.66792]\n",
      " [182.1631 ]\n",
      " [196.66942]\n",
      " [138.37712]]\n",
      "360 Cost:  8.867899 \n",
      "Prediction:\n",
      " [[155.88968]\n",
      " [181.67581]\n",
      " [182.15958]\n",
      " [196.66696]\n",
      " [138.38737]]\n",
      "370 Cost:  8.821178 \n",
      "Prediction:\n",
      " [[155.8782 ]\n",
      " [181.68367]\n",
      " [182.15607]\n",
      " [196.6645 ]\n",
      " [138.39761]]\n",
      "380 Cost:  8.774727 \n",
      "Prediction:\n",
      " [[155.86678]\n",
      " [181.6915 ]\n",
      " [182.15254]\n",
      " [196.66205]\n",
      " [138.40782]]\n",
      "390 Cost:  8.728528 \n",
      "Prediction:\n",
      " [[155.85538]\n",
      " [181.69931]\n",
      " [182.14903]\n",
      " [196.6596 ]\n",
      " [138.418  ]]\n",
      "400 Cost:  8.682584 \n",
      "Prediction:\n",
      " [[155.844  ]\n",
      " [181.70709]\n",
      " [182.14552]\n",
      " [196.65717]\n",
      " [138.42813]]\n",
      "410 Cost:  8.636906 \n",
      "Prediction:\n",
      " [[155.83266]\n",
      " [181.71486]\n",
      " [182.14204]\n",
      " [196.65474]\n",
      " [138.43825]]\n",
      "420 Cost:  8.59141 \n",
      "Prediction:\n",
      " [[155.82132]\n",
      " [181.72261]\n",
      " [182.13857]\n",
      " [196.6523 ]\n",
      " [138.44833]]\n",
      "430 Cost:  8.546209 \n",
      "Prediction:\n",
      " [[155.81004]\n",
      " [181.73033]\n",
      " [182.1351 ]\n",
      " [196.64987]\n",
      " [138.4584 ]]\n",
      "440 Cost:  8.501234 \n",
      "Prediction:\n",
      " [[155.79878]\n",
      " [181.73802]\n",
      " [182.13162]\n",
      " [196.64748]\n",
      " [138.46844]]\n",
      "450 Cost:  8.4564905 \n",
      "Prediction:\n",
      " [[155.78757]\n",
      " [181.74573]\n",
      " [182.12817]\n",
      " [196.64505]\n",
      " [138.47845]]\n",
      "460 Cost:  8.412039 \n",
      "Prediction:\n",
      " [[155.7764 ]\n",
      " [181.7534 ]\n",
      " [182.12476]\n",
      " [196.64268]\n",
      " [138.48845]]\n",
      "470 Cost:  8.367757 \n",
      "Prediction:\n",
      " [[155.7652 ]\n",
      " [181.76103]\n",
      " [182.12129]\n",
      " [196.64027]\n",
      " [138.49838]]\n",
      "480 Cost:  8.3237915 \n",
      "Prediction:\n",
      " [[155.75409]\n",
      " [181.76866]\n",
      " [182.11789]\n",
      " [196.6379 ]\n",
      " [138.50832]]\n",
      "490 Cost:  8.280014 \n",
      "Prediction:\n",
      " [[155.74298]\n",
      " [181.77626]\n",
      " [182.11447]\n",
      " [196.63551]\n",
      " [138.51822]]\n",
      "500 Cost:  8.236522 \n",
      "Prediction:\n",
      " [[155.73192]\n",
      " [181.78383]\n",
      " [182.11107]\n",
      " [196.63315]\n",
      " [138.52809]]\n",
      "510 Cost:  8.193221 \n",
      "Prediction:\n",
      " [[155.72087]\n",
      " [181.7914 ]\n",
      " [182.10768]\n",
      " [196.63077]\n",
      " [138.53793]]\n",
      "520 Cost:  8.150119 \n",
      "Prediction:\n",
      " [[155.70985]\n",
      " [181.79895]\n",
      " [182.10428]\n",
      " [196.62842]\n",
      " [138.54776]]\n",
      "530 Cost:  8.107323 \n",
      "Prediction:\n",
      " [[155.69887]\n",
      " [181.80646]\n",
      " [182.1009 ]\n",
      " [196.62605]\n",
      " [138.55754]]\n",
      "540 Cost:  8.06473 \n",
      "Prediction:\n",
      " [[155.68793]\n",
      " [181.81396]\n",
      " [182.09755]\n",
      " [196.62372]\n",
      " [138.56732]]\n",
      "550 Cost:  8.02236 \n",
      "Prediction:\n",
      " [[155.677  ]\n",
      " [181.82146]\n",
      " [182.09418]\n",
      " [196.62137]\n",
      " [138.57704]]\n",
      "560 Cost:  7.9801993 \n",
      "Prediction:\n",
      " [[155.66609]\n",
      " [181.8289 ]\n",
      " [182.09082]\n",
      " [196.61902]\n",
      " [138.58676]]\n",
      "570 Cost:  7.9383154 \n",
      "Prediction:\n",
      " [[155.65524]\n",
      " [181.83635]\n",
      " [182.08751]\n",
      " [196.61673]\n",
      " [138.59647]]\n",
      "580 Cost:  7.896633 \n",
      "Prediction:\n",
      " [[155.6444 ]\n",
      " [181.84375]\n",
      " [182.08415]\n",
      " [196.61438]\n",
      " [138.60611]]\n",
      "590 Cost:  7.8551683 \n",
      "Prediction:\n",
      " [[155.63359]\n",
      " [181.85117]\n",
      " [182.08083]\n",
      " [196.61208]\n",
      " [138.61574]]\n",
      "600 Cost:  7.813928 \n",
      "Prediction:\n",
      " [[155.6228 ]\n",
      " [181.85854]\n",
      " [182.07751]\n",
      " [196.60977]\n",
      " [138.62535]]\n",
      "610 Cost:  7.772929 \n",
      "Prediction:\n",
      " [[155.61208]\n",
      " [181.8659 ]\n",
      " [182.0742 ]\n",
      " [196.60747]\n",
      " [138.63493]]\n",
      "620 Cost:  7.732095 \n",
      "Prediction:\n",
      " [[155.60133]\n",
      " [181.87325]\n",
      " [182.0709 ]\n",
      " [196.60518]\n",
      " [138.6445 ]]\n",
      "630 Cost:  7.6915894 \n",
      "Prediction:\n",
      " [[155.59065]\n",
      " [181.88054]\n",
      " [182.06763]\n",
      " [196.60289]\n",
      " [138.65402]]\n",
      "640 Cost:  7.651217 \n",
      "Prediction:\n",
      " [[155.57999]\n",
      " [181.88785]\n",
      " [182.06435]\n",
      " [196.6006 ]\n",
      " [138.66353]]\n",
      "650 Cost:  7.611075 \n",
      "Prediction:\n",
      " [[155.56937]\n",
      " [181.89514]\n",
      " [182.06108]\n",
      " [196.59833]\n",
      " [138.67302]]\n",
      "660 Cost:  7.5711794 \n",
      "Prediction:\n",
      " [[155.55875]\n",
      " [181.90239]\n",
      " [182.05782]\n",
      " [196.59605]\n",
      " [138.68245]]\n",
      "670 Cost:  7.5314913 \n",
      "Prediction:\n",
      " [[155.54817]\n",
      " [181.90962]\n",
      " [182.05457]\n",
      " [196.5938 ]\n",
      " [138.69188]]\n",
      "680 Cost:  7.4919906 \n",
      "Prediction:\n",
      " [[155.53764]\n",
      " [181.91689]\n",
      " [182.05135]\n",
      " [196.59157]\n",
      " [138.7013 ]]\n",
      "690 Cost:  7.452699 \n",
      "Prediction:\n",
      " [[155.5271 ]\n",
      " [181.92406]\n",
      " [182.04808]\n",
      " [196.5893 ]\n",
      " [138.71066]]\n",
      "700 Cost:  7.413681 \n",
      "Prediction:\n",
      " [[155.51663]\n",
      " [181.93124]\n",
      " [182.04488]\n",
      " [196.58705]\n",
      " [138.72002]]\n",
      "710 Cost:  7.3748245 \n",
      "Prediction:\n",
      " [[155.50616]\n",
      " [181.9384 ]\n",
      " [182.04164]\n",
      " [196.58482]\n",
      " [138.72934]]\n",
      "720 Cost:  7.3361754 \n",
      "Prediction:\n",
      " [[155.49573]\n",
      " [181.94556]\n",
      " [182.03844]\n",
      " [196.58258]\n",
      " [138.73863]]\n",
      "730 Cost:  7.2977486 \n",
      "Prediction:\n",
      " [[155.48532]\n",
      " [181.95268]\n",
      " [182.03525]\n",
      " [196.58035]\n",
      " [138.74791]]\n",
      "740 Cost:  7.259538 \n",
      "Prediction:\n",
      " [[155.47495]\n",
      " [181.95978]\n",
      " [182.03204]\n",
      " [196.57816]\n",
      " [138.75716]]\n",
      "750 Cost:  7.2215347 \n",
      "Prediction:\n",
      " [[155.4646 ]\n",
      " [181.96686]\n",
      " [182.02887]\n",
      " [196.57594]\n",
      " [138.76639]]\n",
      "760 Cost:  7.1837006 \n",
      "Prediction:\n",
      " [[155.45427]\n",
      " [181.97394]\n",
      " [182.02568]\n",
      " [196.57373]\n",
      " [138.77557]]\n",
      "770 Cost:  7.1461105 \n",
      "Prediction:\n",
      " [[155.44397]\n",
      " [181.98097]\n",
      " [182.02252]\n",
      " [196.57152]\n",
      " [138.78474]]\n",
      "780 Cost:  7.1086893 \n",
      "Prediction:\n",
      " [[155.43372]\n",
      " [181.988  ]\n",
      " [182.01936]\n",
      " [196.56932]\n",
      " [138.79391]]\n",
      "790 Cost:  7.0714874 \n",
      "Prediction:\n",
      " [[155.42346]\n",
      " [181.99501]\n",
      " [182.0162 ]\n",
      " [196.56714]\n",
      " [138.80302]]\n",
      "800 Cost:  7.034503 \n",
      "Prediction:\n",
      " [[155.41327]\n",
      " [182.002  ]\n",
      " [182.01306]\n",
      " [196.56497]\n",
      " [138.81213]]\n",
      "810 Cost:  6.997705 \n",
      "Prediction:\n",
      " [[155.40308]\n",
      " [182.00897]\n",
      " [182.00993]\n",
      " [196.56279]\n",
      " [138.8212 ]]\n",
      "820 Cost:  6.961084 \n",
      "Prediction:\n",
      " [[155.39293]\n",
      " [182.01595]\n",
      " [182.00682]\n",
      " [196.56062]\n",
      " [138.83026]]\n",
      "830 Cost:  6.9247146 \n",
      "Prediction:\n",
      " [[155.3828 ]\n",
      " [182.02287]\n",
      " [182.00371]\n",
      " [196.55846]\n",
      " [138.83926]]\n",
      "840 Cost:  6.8884826 \n",
      "Prediction:\n",
      " [[155.37268]\n",
      " [182.02979]\n",
      " [182.0006 ]\n",
      " [196.55629]\n",
      " [138.84827]]\n",
      "850 Cost:  6.852492 \n",
      "Prediction:\n",
      " [[155.36261]\n",
      " [182.03667]\n",
      " [181.99748]\n",
      " [196.55415]\n",
      " [138.85724]]\n",
      "860 Cost:  6.8166924 \n",
      "Prediction:\n",
      " [[155.35258]\n",
      " [182.04355]\n",
      " [181.99442]\n",
      " [196.552  ]\n",
      " [138.86621]]\n",
      "870 Cost:  6.781039 \n",
      "Prediction:\n",
      " [[155.34254]\n",
      " [182.05042]\n",
      " [181.99132]\n",
      " [196.54985]\n",
      " [138.87512]]\n",
      "880 Cost:  6.7456145 \n",
      "Prediction:\n",
      " [[155.33257]\n",
      " [182.05727]\n",
      " [181.98825]\n",
      " [196.54773]\n",
      " [138.88403]]\n",
      "890 Cost:  6.710355 \n",
      "Prediction:\n",
      " [[155.32259]\n",
      " [182.06409]\n",
      " [181.98518]\n",
      " [196.5456 ]\n",
      " [138.89291]]\n",
      "900 Cost:  6.675341 \n",
      "Prediction:\n",
      " [[155.31265]\n",
      " [182.07088]\n",
      " [181.98213]\n",
      " [196.54347]\n",
      " [138.90176]]\n",
      "910 Cost:  6.6404624 \n",
      "Prediction:\n",
      " [[155.30275]\n",
      " [182.07768]\n",
      " [181.97908]\n",
      " [196.54135]\n",
      " [138.9106 ]]\n",
      "920 Cost:  6.605821 \n",
      "Prediction:\n",
      " [[155.29286]\n",
      " [182.08444]\n",
      " [181.97604]\n",
      " [196.53923]\n",
      " [138.91939]]\n",
      "930 Cost:  6.5713377 \n",
      "Prediction:\n",
      " [[155.283  ]\n",
      " [182.09119]\n",
      " [181.973  ]\n",
      " [196.53714]\n",
      " [138.92818]]\n",
      "940 Cost:  6.537018 \n",
      "Prediction:\n",
      " [[155.27316]\n",
      " [182.09793]\n",
      " [181.96999]\n",
      " [196.53505]\n",
      " [138.93694]]\n",
      "950 Cost:  6.5029097 \n",
      "Prediction:\n",
      " [[155.26337]\n",
      " [182.10463]\n",
      " [181.96695]\n",
      " [196.53296]\n",
      " [138.94568]]\n",
      "960 Cost:  6.46895 \n",
      "Prediction:\n",
      " [[155.25359]\n",
      " [182.11134]\n",
      " [181.96396]\n",
      " [196.53087]\n",
      " [138.9544 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 Cost:  6.435254 \n",
      "Prediction:\n",
      " [[155.24384]\n",
      " [182.11801]\n",
      " [181.96097]\n",
      " [196.5288 ]\n",
      " [138.96307]]\n",
      "980 Cost:  6.4016714 \n",
      "Prediction:\n",
      " [[155.2341 ]\n",
      " [182.12466]\n",
      " [181.95796]\n",
      " [196.52672]\n",
      " [138.97174]]\n",
      "990 Cost:  6.3682756 \n",
      "Prediction:\n",
      " [[155.22441]\n",
      " [182.13132]\n",
      " [181.95497]\n",
      " [196.52466]\n",
      " [138.9804 ]]\n",
      "1000 Cost:  6.3350854 \n",
      "Prediction:\n",
      " [[155.21472]\n",
      " [182.13794]\n",
      " [181.952  ]\n",
      " [196.52258]\n",
      " [138.98898]]\n",
      "1010 Cost:  6.302095 \n",
      "Prediction:\n",
      " [[155.20508]\n",
      " [182.14453]\n",
      " [181.94904]\n",
      " [196.52052]\n",
      " [138.99757]]\n",
      "1020 Cost:  6.269219 \n",
      "Prediction:\n",
      " [[155.19545]\n",
      " [182.15112]\n",
      " [181.94608]\n",
      " [196.5185 ]\n",
      " [139.00616]]\n",
      "1030 Cost:  6.2365694 \n",
      "Prediction:\n",
      " [[155.18585]\n",
      " [182.15768]\n",
      " [181.94312]\n",
      " [196.51642]\n",
      " [139.0147 ]]\n",
      "1040 Cost:  6.2041006 \n",
      "Prediction:\n",
      " [[155.17628]\n",
      " [182.16423]\n",
      " [181.94017]\n",
      " [196.51437]\n",
      " [139.02321]]\n",
      "1050 Cost:  6.1717477 \n",
      "Prediction:\n",
      " [[155.16675]\n",
      " [182.17079]\n",
      " [181.93723]\n",
      " [196.51236]\n",
      " [139.03172]]\n",
      "1060 Cost:  6.1396413 \n",
      "Prediction:\n",
      " [[155.15723]\n",
      " [182.17729]\n",
      " [181.93431]\n",
      " [196.51031]\n",
      " [139.04019]]\n",
      "1070 Cost:  6.10767 \n",
      "Prediction:\n",
      " [[155.14774]\n",
      " [182.18379]\n",
      " [181.93138]\n",
      " [196.50829]\n",
      " [139.04865]]\n",
      "1080 Cost:  6.075873 \n",
      "Prediction:\n",
      " [[155.13826]\n",
      " [182.19028]\n",
      " [181.92847]\n",
      " [196.50627]\n",
      " [139.05707]]\n",
      "1090 Cost:  6.044249 \n",
      "Prediction:\n",
      " [[155.12881]\n",
      " [182.19675]\n",
      " [181.92557]\n",
      " [196.50427]\n",
      " [139.06548]]\n",
      "1100 Cost:  6.0128016 \n",
      "Prediction:\n",
      " [[155.1194 ]\n",
      " [182.20319]\n",
      " [181.92267]\n",
      " [196.50227]\n",
      " [139.07387]]\n",
      "1110 Cost:  5.9815187 \n",
      "Prediction:\n",
      " [[155.11   ]\n",
      " [182.20961]\n",
      " [181.91978]\n",
      " [196.50026]\n",
      " [139.08223]]\n",
      "1120 Cost:  5.9504027 \n",
      "Prediction:\n",
      " [[155.10065]\n",
      " [182.21603]\n",
      " [181.9169 ]\n",
      " [196.49828]\n",
      " [139.09058]]\n",
      "1130 Cost:  5.9194884 \n",
      "Prediction:\n",
      " [[155.09132]\n",
      " [182.22243]\n",
      " [181.91403]\n",
      " [196.49629]\n",
      " [139.09889]]\n",
      "1140 Cost:  5.888711 \n",
      "Prediction:\n",
      " [[155.082  ]\n",
      " [182.2288 ]\n",
      " [181.91116]\n",
      " [196.49431]\n",
      " [139.10718]]\n",
      "1150 Cost:  5.8581176 \n",
      "Prediction:\n",
      " [[155.07272]\n",
      " [182.23515]\n",
      " [181.90831]\n",
      " [196.49232]\n",
      " [139.11546]]\n",
      "1160 Cost:  5.8276434 \n",
      "Prediction:\n",
      " [[155.06345]\n",
      " [182.24152]\n",
      " [181.90546]\n",
      " [196.49034]\n",
      " [139.1237 ]]\n",
      "1170 Cost:  5.797374 \n",
      "Prediction:\n",
      " [[155.05421]\n",
      " [182.24785]\n",
      " [181.90263]\n",
      " [196.4884 ]\n",
      " [139.13194]]\n",
      "1180 Cost:  5.7672734 \n",
      "Prediction:\n",
      " [[155.04498]\n",
      " [182.25412]\n",
      " [181.89978]\n",
      " [196.48642]\n",
      " [139.14014]]\n",
      "1190 Cost:  5.7373047 \n",
      "Prediction:\n",
      " [[155.03581]\n",
      " [182.26045]\n",
      " [181.89697]\n",
      " [196.48448]\n",
      " [139.14833]]\n",
      "1200 Cost:  5.707476 \n",
      "Prediction:\n",
      " [[155.02661]\n",
      " [182.26671]\n",
      " [181.89412]\n",
      " [196.48251]\n",
      " [139.15648]]\n",
      "1210 Cost:  5.677877 \n",
      "Prediction:\n",
      " [[155.01749]\n",
      " [182.27296]\n",
      " [181.89131]\n",
      " [196.48058]\n",
      " [139.16461]]\n",
      "1220 Cost:  5.6483903 \n",
      "Prediction:\n",
      " [[155.00836]\n",
      " [182.2792 ]\n",
      " [181.8885 ]\n",
      " [196.47862]\n",
      " [139.17273]]\n",
      "1230 Cost:  5.6191354 \n",
      "Prediction:\n",
      " [[154.99927]\n",
      " [182.2854 ]\n",
      " [181.88571]\n",
      " [196.47668]\n",
      " [139.1808 ]]\n",
      "1240 Cost:  5.5899663 \n",
      "Prediction:\n",
      " [[154.99022]\n",
      " [182.29163]\n",
      " [181.88292]\n",
      " [196.47476]\n",
      " [139.18889]]\n",
      "1250 Cost:  5.5609064 \n",
      "Prediction:\n",
      " [[154.98116]\n",
      " [182.29784]\n",
      " [181.88013]\n",
      " [196.47282]\n",
      " [139.19695]]\n",
      "1260 Cost:  5.5321097 \n",
      "Prediction:\n",
      " [[154.97215]\n",
      " [182.304  ]\n",
      " [181.87737]\n",
      " [196.47092]\n",
      " [139.20497]]\n",
      "1270 Cost:  5.5034227 \n",
      "Prediction:\n",
      " [[154.96315]\n",
      " [182.31015]\n",
      " [181.8746 ]\n",
      " [196.469  ]\n",
      " [139.21298]]\n",
      "1280 Cost:  5.474893 \n",
      "Prediction:\n",
      " [[154.95418]\n",
      " [182.3163 ]\n",
      " [181.87183]\n",
      " [196.4671 ]\n",
      " [139.22095]]\n",
      "1290 Cost:  5.4465137 \n",
      "Prediction:\n",
      " [[154.94524]\n",
      " [182.32243]\n",
      " [181.86908]\n",
      " [196.4652 ]\n",
      " [139.22891]]\n",
      "1300 Cost:  5.4183245 \n",
      "Prediction:\n",
      " [[154.93633]\n",
      " [182.32852]\n",
      " [181.86635]\n",
      " [196.46329]\n",
      " [139.23686]]\n",
      "1310 Cost:  5.390254 \n",
      "Prediction:\n",
      " [[154.92743]\n",
      " [182.33461]\n",
      " [181.8636 ]\n",
      " [196.4614 ]\n",
      " [139.24478]]\n",
      "1320 Cost:  5.36237 \n",
      "Prediction:\n",
      " [[154.91855]\n",
      " [182.34068]\n",
      " [181.86089]\n",
      " [196.45952]\n",
      " [139.25266]]\n",
      "1330 Cost:  5.3345976 \n",
      "Prediction:\n",
      " [[154.9097 ]\n",
      " [182.34673]\n",
      " [181.85815]\n",
      " [196.45763]\n",
      " [139.26054]]\n",
      "1340 Cost:  5.3069425 \n",
      "Prediction:\n",
      " [[154.90086]\n",
      " [182.3528 ]\n",
      " [181.85542]\n",
      " [196.45576]\n",
      " [139.26839]]\n",
      "1350 Cost:  5.2794995 \n",
      "Prediction:\n",
      " [[154.89207]\n",
      " [182.35881]\n",
      " [181.85272]\n",
      " [196.45389]\n",
      " [139.27623]]\n",
      "1360 Cost:  5.252137 \n",
      "Prediction:\n",
      " [[154.88327]\n",
      " [182.36482]\n",
      " [181.85   ]\n",
      " [196.45203]\n",
      " [139.28404]]\n",
      "1370 Cost:  5.224992 \n",
      "Prediction:\n",
      " [[154.87454]\n",
      " [182.37082]\n",
      " [181.84732]\n",
      " [196.45016]\n",
      " [139.29184]]\n",
      "1380 Cost:  5.1979704 \n",
      "Prediction:\n",
      " [[154.8658 ]\n",
      " [182.37679]\n",
      " [181.84464]\n",
      " [196.44832]\n",
      " [139.2996 ]]\n",
      "1390 Cost:  5.171073 \n",
      "Prediction:\n",
      " [[154.8571 ]\n",
      " [182.38277]\n",
      " [181.84195]\n",
      " [196.44646]\n",
      " [139.30736]]\n",
      "1400 Cost:  5.144376 \n",
      "Prediction:\n",
      " [[154.84842]\n",
      " [182.38869]\n",
      " [181.83928]\n",
      " [196.44461]\n",
      " [139.31508]]\n",
      "1410 Cost:  5.117782 \n",
      "Prediction:\n",
      " [[154.83975]\n",
      " [182.39464]\n",
      " [181.83662]\n",
      " [196.44278]\n",
      " [139.32277]]\n",
      "1420 Cost:  5.091329 \n",
      "Prediction:\n",
      " [[154.83112]\n",
      " [182.40054]\n",
      " [181.83397]\n",
      " [196.44096]\n",
      " [139.33047]]\n",
      "1430 Cost:  5.0650277 \n",
      "Prediction:\n",
      " [[154.8225 ]\n",
      " [182.40643]\n",
      " [181.83131]\n",
      " [196.43913]\n",
      " [139.33813]]\n",
      "1440 Cost:  5.038864 \n",
      "Prediction:\n",
      " [[154.8139 ]\n",
      " [182.41231]\n",
      " [181.82866]\n",
      " [196.4373 ]\n",
      " [139.34578]]\n",
      "1450 Cost:  5.0128503 \n",
      "Prediction:\n",
      " [[154.80533]\n",
      " [182.41818]\n",
      " [181.82602]\n",
      " [196.43549]\n",
      " [139.35338]]\n",
      "1460 Cost:  4.9869795 \n",
      "Prediction:\n",
      " [[154.7968 ]\n",
      " [182.42404]\n",
      " [181.82341]\n",
      " [196.43367]\n",
      " [139.361  ]]\n",
      "1470 Cost:  4.9612365 \n",
      "Prediction:\n",
      " [[154.78827]\n",
      " [182.42986]\n",
      " [181.82077]\n",
      " [196.43185]\n",
      " [139.36858]]\n",
      "1480 Cost:  4.935626 \n",
      "Prediction:\n",
      " [[154.77975]\n",
      " [182.43567]\n",
      " [181.81815]\n",
      " [196.43005]\n",
      " [139.37613]]\n",
      "1490 Cost:  4.9101624 \n",
      "Prediction:\n",
      " [[154.77129]\n",
      " [182.44148]\n",
      " [181.81554]\n",
      " [196.42825]\n",
      " [139.38367]]\n",
      "1500 Cost:  4.8848495 \n",
      "Prediction:\n",
      " [[154.76283]\n",
      " [182.44725]\n",
      " [181.81293]\n",
      " [196.42647]\n",
      " [139.39119]]\n",
      "1510 Cost:  4.8596625 \n",
      "Prediction:\n",
      " [[154.75443]\n",
      " [182.45305]\n",
      " [181.81035]\n",
      " [196.42468]\n",
      " [139.3987 ]]\n",
      "1520 Cost:  4.834632 \n",
      "Prediction:\n",
      " [[154.746  ]\n",
      " [182.45879]\n",
      " [181.80779]\n",
      " [196.4229 ]\n",
      " [139.40617]]\n",
      "1530 Cost:  4.8097067 \n",
      "Prediction:\n",
      " [[154.73763]\n",
      " [182.46452]\n",
      " [181.80516]\n",
      " [196.42111]\n",
      " [139.41362]]\n",
      "1540 Cost:  4.7849197 \n",
      "Prediction:\n",
      " [[154.72925]\n",
      " [182.47025]\n",
      " [181.80261]\n",
      " [196.41933]\n",
      " [139.42107]]\n",
      "1550 Cost:  4.7602906 \n",
      "Prediction:\n",
      " [[154.72092]\n",
      " [182.47595]\n",
      " [181.80005]\n",
      " [196.41757]\n",
      " [139.42848]]\n",
      "1560 Cost:  4.7357597 \n",
      "Prediction:\n",
      " [[154.7126 ]\n",
      " [182.48166]\n",
      " [181.79749]\n",
      " [196.41582]\n",
      " [139.43588]]\n",
      "1570 Cost:  4.711398 \n",
      "Prediction:\n",
      " [[154.7043 ]\n",
      " [182.48732]\n",
      " [181.79494]\n",
      " [196.41405]\n",
      " [139.44325]]\n",
      "1580 Cost:  4.687138 \n",
      "Prediction:\n",
      " [[154.69603]\n",
      " [182.493  ]\n",
      " [181.79239]\n",
      " [196.4123 ]\n",
      " [139.4506 ]]\n",
      "1590 Cost:  4.6629972 \n",
      "Prediction:\n",
      " [[154.68777]\n",
      " [182.49864]\n",
      " [181.78983]\n",
      " [196.41055]\n",
      " [139.45795]]\n",
      "1600 Cost:  4.6390347 \n",
      "Prediction:\n",
      " [[154.67957]\n",
      " [182.50427]\n",
      " [181.78731]\n",
      " [196.40881]\n",
      " [139.46527]]\n",
      "1610 Cost:  4.615184 \n",
      "Prediction:\n",
      " [[154.67136]\n",
      " [182.50989]\n",
      " [181.78479]\n",
      " [196.40707]\n",
      " [139.47256]]\n",
      "1620 Cost:  4.591472 \n",
      "Prediction:\n",
      " [[154.66318]\n",
      " [182.51549]\n",
      " [181.78227]\n",
      " [196.40532]\n",
      " [139.47983]]\n",
      "1630 Cost:  4.567853 \n",
      "Prediction:\n",
      " [[154.65501]\n",
      " [182.52107]\n",
      " [181.77974]\n",
      " [196.40361]\n",
      " [139.48709]]\n",
      "1640 Cost:  4.5443726 \n",
      "Prediction:\n",
      " [[154.64687]\n",
      " [182.52664]\n",
      " [181.77725]\n",
      " [196.40187]\n",
      " [139.49434]]\n",
      "1650 Cost:  4.5210238 \n",
      "Prediction:\n",
      " [[154.63873]\n",
      " [182.5322 ]\n",
      " [181.77475]\n",
      " [196.40015]\n",
      " [139.50154]]\n",
      "1660 Cost:  4.4978027 \n",
      "Prediction:\n",
      " [[154.63066]\n",
      " [182.53775]\n",
      " [181.77226]\n",
      " [196.39845]\n",
      " [139.50876]]\n",
      "1670 Cost:  4.474731 \n",
      "Prediction:\n",
      " [[154.62259]\n",
      " [182.54327]\n",
      " [181.76978]\n",
      " [196.39673]\n",
      " [139.51591]]\n",
      "1680 Cost:  4.451727 \n",
      "Prediction:\n",
      " [[154.6145 ]\n",
      " [182.54877]\n",
      " [181.76727]\n",
      " [196.39502]\n",
      " [139.52307]]\n",
      "1690 Cost:  4.4288983 \n",
      "Prediction:\n",
      " [[154.6065 ]\n",
      " [182.55429]\n",
      " [181.76483]\n",
      " [196.39334]\n",
      " [139.53023]]\n",
      "1700 Cost:  4.4061794 \n",
      "Prediction:\n",
      " [[154.5985 ]\n",
      " [182.55977]\n",
      " [181.76236]\n",
      " [196.39163]\n",
      " [139.53734]]\n",
      "1710 Cost:  4.383556 \n",
      "Prediction:\n",
      " [[154.5905 ]\n",
      " [182.56525]\n",
      " [181.7599 ]\n",
      " [196.38997]\n",
      " [139.54445]]\n",
      "1720 Cost:  4.361104 \n",
      "Prediction:\n",
      " [[154.58252]\n",
      " [182.57066]\n",
      " [181.75745]\n",
      " [196.38824]\n",
      " [139.55151]]\n",
      "1730 Cost:  4.3387513 \n",
      "Prediction:\n",
      " [[154.5746 ]\n",
      " [182.57613]\n",
      " [181.755  ]\n",
      " [196.38658]\n",
      " [139.55858]]\n",
      "1740 Cost:  4.3164897 \n",
      "Prediction:\n",
      " [[154.56667]\n",
      " [182.58154]\n",
      " [181.75255]\n",
      " [196.3849 ]\n",
      " [139.56563]]\n",
      "1750 Cost:  4.2944 \n",
      "Prediction:\n",
      " [[154.55876]\n",
      " [182.58694]\n",
      " [181.75012]\n",
      " [196.38322]\n",
      " [139.57263]]\n",
      "1760 Cost:  4.272403 \n",
      "Prediction:\n",
      " [[154.55087]\n",
      " [182.59233]\n",
      " [181.7477 ]\n",
      " [196.38156]\n",
      " [139.57964]]\n",
      "1770 Cost:  4.250468 \n",
      "Prediction:\n",
      " [[154.54301]\n",
      " [182.59773]\n",
      " [181.74527]\n",
      " [196.3799 ]\n",
      " [139.58665]]\n",
      "1780 Cost:  4.228737 \n",
      "Prediction:\n",
      " [[154.53517]\n",
      " [182.60309]\n",
      " [181.74286]\n",
      " [196.37825]\n",
      " [139.5936 ]]\n",
      "1790 Cost:  4.207109 \n",
      "Prediction:\n",
      " [[154.52737]\n",
      " [182.60843]\n",
      " [181.74045]\n",
      " [196.37659]\n",
      " [139.60056]]\n",
      "1800 Cost:  4.1855664 \n",
      "Prediction:\n",
      " [[154.51956]\n",
      " [182.61377]\n",
      " [181.73805]\n",
      " [196.37494]\n",
      " [139.60748]]\n",
      "1810 Cost:  4.1641755 \n",
      "Prediction:\n",
      " [[154.5118 ]\n",
      " [182.61908]\n",
      " [181.73564]\n",
      " [196.37329]\n",
      " [139.61438]]\n",
      "1820 Cost:  4.1428776 \n",
      "Prediction:\n",
      " [[154.50404]\n",
      " [182.6244 ]\n",
      " [181.73328]\n",
      " [196.37166]\n",
      " [139.62128]]\n",
      "1830 Cost:  4.1217146 \n",
      "Prediction:\n",
      " [[154.4963 ]\n",
      " [182.62968]\n",
      " [181.7309 ]\n",
      " [196.37003]\n",
      " [139.62814]]\n",
      "1840 Cost:  4.1006513 \n",
      "Prediction:\n",
      " [[154.4886 ]\n",
      " [182.63498]\n",
      " [181.72853]\n",
      " [196.36841]\n",
      " [139.635  ]]\n",
      "1850 Cost:  4.07968 \n",
      "Prediction:\n",
      " [[154.4809 ]\n",
      " [182.64023]\n",
      " [181.72614]\n",
      " [196.36678]\n",
      " [139.64183]]\n",
      "1860 Cost:  4.058843 \n",
      "Prediction:\n",
      " [[154.47322]\n",
      " [182.64548]\n",
      " [181.72379]\n",
      " [196.36516]\n",
      " [139.64865]]\n",
      "1870 Cost:  4.0381236 \n",
      "Prediction:\n",
      " [[154.46558]\n",
      " [182.65071]\n",
      " [181.72142]\n",
      " [196.36353]\n",
      " [139.65544]]\n",
      "1880 Cost:  4.0174923 \n",
      "Prediction:\n",
      " [[154.45795]\n",
      " [182.65594]\n",
      " [181.71907]\n",
      " [196.36194]\n",
      " [139.66223]]\n",
      "1890 Cost:  3.9970078 \n",
      "Prediction:\n",
      " [[154.45035]\n",
      " [182.66116]\n",
      " [181.71675]\n",
      " [196.36034]\n",
      " [139.66899]]\n",
      "1900 Cost:  3.976603 \n",
      "Prediction:\n",
      " [[154.44275]\n",
      " [182.66634]\n",
      " [181.71439]\n",
      " [196.35872]\n",
      " [139.67574]]\n",
      "1910 Cost:  3.95633 \n",
      "Prediction:\n",
      " [[154.43518]\n",
      " [182.67152]\n",
      " [181.71207]\n",
      " [196.35712]\n",
      " [139.68245]]\n",
      "1920 Cost:  3.936145 \n",
      "Prediction:\n",
      " [[154.42763]\n",
      " [182.6767 ]\n",
      " [181.70973]\n",
      " [196.35551]\n",
      " [139.68915]]\n",
      "1930 Cost:  3.9160976 \n",
      "Prediction:\n",
      " [[154.4201 ]\n",
      " [182.68184]\n",
      " [181.70743]\n",
      " [196.35394]\n",
      " [139.69585]]\n",
      "1940 Cost:  3.8961601 \n",
      "Prediction:\n",
      " [[154.4126 ]\n",
      " [182.68697]\n",
      " [181.70511]\n",
      " [196.35234]\n",
      " [139.7025 ]]\n",
      "1950 Cost:  3.8763204 \n",
      "Prediction:\n",
      " [[154.40512]\n",
      " [182.6921 ]\n",
      " [181.70282]\n",
      " [196.35078]\n",
      " [139.70917]]\n",
      "1960 Cost:  3.856578 \n",
      "Prediction:\n",
      " [[154.39764]\n",
      " [182.6972 ]\n",
      " [181.7005 ]\n",
      " [196.3492 ]\n",
      " [139.71577]]\n",
      "1970 Cost:  3.836928 \n",
      "Prediction:\n",
      " [[154.3902 ]\n",
      " [182.7023 ]\n",
      " [181.69821]\n",
      " [196.34763]\n",
      " [139.72241]]\n",
      "1980 Cost:  3.8174157 \n",
      "Prediction:\n",
      " [[154.38278]\n",
      " [182.70738]\n",
      " [181.69592]\n",
      " [196.34605]\n",
      " [139.729  ]]\n",
      "1990 Cost:  3.797966 \n",
      "Prediction:\n",
      " [[154.37535]\n",
      " [182.71246]\n",
      " [181.69365]\n",
      " [196.34448]\n",
      " [139.73558]]\n",
      "2000 Cost:  3.7786713 \n",
      "Prediction:\n",
      " [[154.36797]\n",
      " [182.71751]\n",
      " [181.69138]\n",
      " [196.34294]\n",
      " [139.74213]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3) [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]] 6\n",
      "(6, 1) [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]]\n"
     ]
    }
   ],
   "source": [
    "## 파일 불러와서 작업하기 연습\n",
    "import numpy as np\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1] # 0:-1은 마지막 열을 제외하고 모두 가져온다는 뜻\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data)\n",
    "\n",
    "# placeholders for a tensor that wil be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  56666.973 \n",
      "Prediction:\n",
      " [[-68.10884 ]\n",
      " [-86.76683 ]\n",
      " [-83.03737 ]\n",
      " [-90.21883 ]\n",
      " [-67.44865 ]\n",
      " [-49.725338]]\n",
      "10 Cost:  10.596723 \n",
      "Prediction:\n",
      " [[153.7915 ]\n",
      " [180.03546]\n",
      " [179.79752]\n",
      " [196.01566]\n",
      " [136.06647]\n",
      " [100.30997]]\n",
      "20 Cost:  7.9660964 \n",
      "Prediction:\n",
      " [[155.28418 ]\n",
      " [181.84761 ]\n",
      " [181.57368 ]\n",
      " [197.95065 ]\n",
      " [137.45253 ]\n",
      " [101.334915]]\n",
      "30 Cost:  7.93425 \n",
      "Prediction:\n",
      " [[155.28416]\n",
      " [181.86507]\n",
      " [181.58183]\n",
      " [197.96022]\n",
      " [137.46962]\n",
      " [101.35064]]\n",
      "40 Cost:  7.9026895 \n",
      "Prediction:\n",
      " [[155.27406]\n",
      " [181.87038]\n",
      " [181.57802]\n",
      " [197.95677]\n",
      " [137.47742]\n",
      " [101.35952]]\n",
      "50 Cost:  7.871338 \n",
      "Prediction:\n",
      " [[155.26395]\n",
      " [181.87558]\n",
      " [181.57414]\n",
      " [197.95323]\n",
      " [137.48514]\n",
      " [101.36832]]\n",
      "60 Cost:  7.8401947 \n",
      "Prediction:\n",
      " [[155.25386]\n",
      " [181.88077]\n",
      " [181.57027]\n",
      " [197.94972]\n",
      " [137.49283]\n",
      " [101.37709]]\n",
      "70 Cost:  7.8092327 \n",
      "Prediction:\n",
      " [[155.24379]\n",
      " [181.88593]\n",
      " [181.5664 ]\n",
      " [197.9462 ]\n",
      " [137.50049]\n",
      " [101.3858 ]]\n",
      "80 Cost:  7.7784595 \n",
      "Prediction:\n",
      " [[155.23378 ]\n",
      " [181.89108 ]\n",
      " [181.56258 ]\n",
      " [197.94269 ]\n",
      " [137.50815 ]\n",
      " [101.394516]]\n",
      "90 Cost:  7.7478733 \n",
      "Prediction:\n",
      " [[155.22379]\n",
      " [181.89622]\n",
      " [181.55875]\n",
      " [197.93921]\n",
      " [137.51578]\n",
      " [101.40318]]\n",
      "100 Cost:  7.7174797 \n",
      "Prediction:\n",
      " [[155.21384]\n",
      " [181.90135]\n",
      " [181.55492]\n",
      " [197.93571]\n",
      " [137.52338]\n",
      " [101.41182]]\n",
      "110 Cost:  7.687285 \n",
      "Prediction:\n",
      " [[155.20392]\n",
      " [181.90645]\n",
      " [181.55113]\n",
      " [197.93222]\n",
      " [137.53096]\n",
      " [101.42042]]\n",
      "120 Cost:  7.657264 \n",
      "Prediction:\n",
      " [[155.19402]\n",
      " [181.91154]\n",
      " [181.54735]\n",
      " [197.92876]\n",
      " [137.53851]\n",
      " [101.429  ]]\n",
      "130 Cost:  7.6274605 \n",
      "Prediction:\n",
      " [[155.18417 ]\n",
      " [181.91661 ]\n",
      " [181.54356 ]\n",
      " [197.92531 ]\n",
      " [137.54605 ]\n",
      " [101.437546]]\n",
      "140 Cost:  7.5978146 \n",
      "Prediction:\n",
      " [[155.17436]\n",
      " [181.92168]\n",
      " [181.53981]\n",
      " [197.92186]\n",
      " [137.55357]\n",
      " [101.44606]]\n",
      "150 Cost:  7.568348 \n",
      "Prediction:\n",
      " [[155.16455]\n",
      " [181.9267 ]\n",
      " [181.53606]\n",
      " [197.91841]\n",
      " [137.56107]\n",
      " [101.45454]]\n",
      "160 Cost:  7.539061 \n",
      "Prediction:\n",
      " [[155.1548 ]\n",
      " [181.93175]\n",
      " [181.53233]\n",
      " [197.915  ]\n",
      " [137.56854]\n",
      " [101.463  ]]\n",
      "170 Cost:  7.5099792 \n",
      "Prediction:\n",
      " [[155.14508 ]\n",
      " [181.93674 ]\n",
      " [181.5286  ]\n",
      " [197.91156 ]\n",
      " [137.57599 ]\n",
      " [101.471405]]\n",
      "180 Cost:  7.4810753 \n",
      "Prediction:\n",
      " [[155.13539 ]\n",
      " [181.94173 ]\n",
      " [181.52489 ]\n",
      " [197.90816 ]\n",
      " [137.58342 ]\n",
      " [101.479805]]\n",
      "190 Cost:  7.452326 \n",
      "Prediction:\n",
      " [[155.12573]\n",
      " [181.94672]\n",
      " [181.52121]\n",
      " [197.90475]\n",
      " [137.59084]\n",
      " [101.48817]]\n",
      "200 Cost:  7.423802 \n",
      "Prediction:\n",
      " [[155.11612]\n",
      " [181.95168]\n",
      " [181.51753]\n",
      " [197.90138]\n",
      " [137.59822]\n",
      " [101.4965 ]]\n",
      "210 Cost:  7.395392 \n",
      "Prediction:\n",
      " [[155.10649]\n",
      " [181.95662]\n",
      " [181.51384]\n",
      " [197.89796]\n",
      " [137.60556]\n",
      " [101.5048 ]]\n",
      "220 Cost:  7.3672047 \n",
      "Prediction:\n",
      " [[155.09694]\n",
      " [181.96155]\n",
      " [181.51018]\n",
      " [197.89459]\n",
      " [137.6129 ]\n",
      " [101.51307]]\n",
      "230 Cost:  7.339182 \n",
      "Prediction:\n",
      " [[155.0874 ]\n",
      " [181.96645]\n",
      " [181.50655]\n",
      " [197.89122]\n",
      " [137.62022]\n",
      " [101.5213 ]]\n",
      "240 Cost:  7.311335 \n",
      "Prediction:\n",
      " [[155.07791]\n",
      " [181.97134]\n",
      " [181.5029 ]\n",
      " [197.88786]\n",
      " [137.62752]\n",
      " [101.52951]]\n",
      "250 Cost:  7.2836494 \n",
      "Prediction:\n",
      " [[155.06844]\n",
      " [181.97623]\n",
      " [181.49928]\n",
      " [197.88452]\n",
      " [137.6348 ]\n",
      " [101.5377 ]]\n",
      "260 Cost:  7.256145 \n",
      "Prediction:\n",
      " [[155.05902]\n",
      " [181.98111]\n",
      " [181.49568]\n",
      " [197.8812 ]\n",
      " [137.64206]\n",
      " [101.54585]]\n",
      "270 Cost:  7.2288 \n",
      "Prediction:\n",
      " [[155.0496 ]\n",
      " [181.98595]\n",
      " [181.49208]\n",
      " [197.87785]\n",
      " [137.64929]\n",
      " [101.55397]]\n",
      "280 Cost:  7.2016215 \n",
      "Prediction:\n",
      " [[155.04025 ]\n",
      " [181.99078 ]\n",
      " [181.48851 ]\n",
      " [197.87453 ]\n",
      " [137.65652 ]\n",
      " [101.562065]]\n",
      "290 Cost:  7.1746125 \n",
      "Prediction:\n",
      " [[155.03088]\n",
      " [181.9956 ]\n",
      " [181.48492]\n",
      " [197.87122]\n",
      " [137.6637 ]\n",
      " [101.57014]]\n",
      "300 Cost:  7.147802 \n",
      "Prediction:\n",
      " [[155.02159]\n",
      " [182.0004 ]\n",
      " [181.48137]\n",
      " [197.8679 ]\n",
      " [137.67087]\n",
      " [101.57816]]\n",
      "310 Cost:  7.121116 \n",
      "Prediction:\n",
      " [[155.0123 ]\n",
      " [182.00519]\n",
      " [181.47784]\n",
      " [197.86461]\n",
      " [137.67802]\n",
      " [101.58617]]\n",
      "320 Cost:  7.0945945 \n",
      "Prediction:\n",
      " [[155.00305 ]\n",
      " [182.00996 ]\n",
      " [181.4743  ]\n",
      " [197.86133 ]\n",
      " [137.68517 ]\n",
      " [101.594154]]\n",
      "330 Cost:  7.068264 \n",
      "Prediction:\n",
      " [[154.99384]\n",
      " [182.01471]\n",
      " [181.4708 ]\n",
      " [197.85805]\n",
      " [137.69228]\n",
      " [101.60209]]\n",
      "340 Cost:  7.042073 \n",
      "Prediction:\n",
      " [[154.98465 ]\n",
      " [182.01947 ]\n",
      " [181.46729 ]\n",
      " [197.85477 ]\n",
      " [137.69936 ]\n",
      " [101.610016]]\n",
      "350 Cost:  7.0160794 \n",
      "Prediction:\n",
      " [[154.9755 ]\n",
      " [182.02419]\n",
      " [181.46379]\n",
      " [197.85152]\n",
      " [137.70642]\n",
      " [101.61791]]\n",
      "360 Cost:  6.990209 \n",
      "Prediction:\n",
      " [[154.96637]\n",
      " [182.0289 ]\n",
      " [181.4603 ]\n",
      " [197.84827]\n",
      " [137.71347]\n",
      " [101.62576]]\n",
      "370 Cost:  6.9645133 \n",
      "Prediction:\n",
      " [[154.95729 ]\n",
      " [182.0336  ]\n",
      " [181.45683 ]\n",
      " [197.84502 ]\n",
      " [137.7205  ]\n",
      " [101.633606]]\n",
      "380 Cost:  6.9389725 \n",
      "Prediction:\n",
      " [[154.94821]\n",
      " [182.03828]\n",
      " [181.45338]\n",
      " [197.8418 ]\n",
      " [137.72751]\n",
      " [101.6414 ]]\n",
      "390 Cost:  6.9135776 \n",
      "Prediction:\n",
      " [[154.93918 ]\n",
      " [182.04294 ]\n",
      " [181.44992 ]\n",
      " [197.83855 ]\n",
      " [137.7345  ]\n",
      " [101.649185]]\n",
      "400 Cost:  6.8883357 \n",
      "Prediction:\n",
      " [[154.93018]\n",
      " [182.04759]\n",
      " [181.44649]\n",
      " [197.83533]\n",
      " [137.74147]\n",
      " [101.65692]]\n",
      "410 Cost:  6.8632646 \n",
      "Prediction:\n",
      " [[154.9212 ]\n",
      " [182.05225]\n",
      " [181.44308]\n",
      " [197.83212]\n",
      " [137.74841]\n",
      " [101.66464]]\n",
      "420 Cost:  6.8383727 \n",
      "Prediction:\n",
      " [[154.91228]\n",
      " [182.05687]\n",
      " [181.43968]\n",
      " [197.82893]\n",
      " [137.75534]\n",
      " [101.67234]]\n",
      "430 Cost:  6.813606 \n",
      "Prediction:\n",
      " [[154.90335]\n",
      " [182.06146]\n",
      " [181.43626]\n",
      " [197.82573]\n",
      " [137.76224]\n",
      " [101.68001]]\n",
      "440 Cost:  6.788975 \n",
      "Prediction:\n",
      " [[154.89447]\n",
      " [182.06606]\n",
      " [181.43288]\n",
      " [197.82254]\n",
      " [137.76913]\n",
      " [101.68764]]\n",
      "450 Cost:  6.7645245 \n",
      "Prediction:\n",
      " [[154.88562 ]\n",
      " [182.07063 ]\n",
      " [181.42952 ]\n",
      " [197.81937 ]\n",
      " [137.776   ]\n",
      " [101.695244]]\n",
      "460 Cost:  6.7401752 \n",
      "Prediction:\n",
      " [[154.87679 ]\n",
      " [182.07521 ]\n",
      " [181.42615 ]\n",
      " [197.8162  ]\n",
      " [137.78285 ]\n",
      " [101.702835]]\n",
      "470 Cost:  6.716062 \n",
      "Prediction:\n",
      " [[154.86801]\n",
      " [182.07974]\n",
      " [181.42279]\n",
      " [197.81305]\n",
      " [137.78967]\n",
      " [101.71038]]\n",
      "480 Cost:  6.6920304 \n",
      "Prediction:\n",
      " [[154.85924]\n",
      " [182.08429]\n",
      " [181.41946]\n",
      " [197.80989]\n",
      " [137.79648]\n",
      " [101.71791]]\n",
      "490 Cost:  6.668171 \n",
      "Prediction:\n",
      " [[154.85052]\n",
      " [182.08882]\n",
      " [181.41617]\n",
      " [197.80676]\n",
      " [137.80328]\n",
      " [101.72541]]\n",
      "500 Cost:  6.6444516 \n",
      "Prediction:\n",
      " [[154.8418 ]\n",
      " [182.0933 ]\n",
      " [181.41283]\n",
      " [197.8036 ]\n",
      " [137.81003]\n",
      " [101.73288]]\n",
      "510 Cost:  6.6208615 \n",
      "Prediction:\n",
      " [[154.83311]\n",
      " [182.0978 ]\n",
      " [181.40953]\n",
      " [197.80048]\n",
      " [137.81679]\n",
      " [101.74032]]\n",
      "520 Cost:  6.597428 \n",
      "Prediction:\n",
      " [[154.8245 ]\n",
      " [182.1023 ]\n",
      " [181.40625]\n",
      " [197.79738]\n",
      " [137.82353]\n",
      " [101.74774]]\n",
      "530 Cost:  6.5741253 \n",
      "Prediction:\n",
      " [[154.81586]\n",
      " [182.10675]\n",
      " [181.40295]\n",
      " [197.79425]\n",
      " [137.83023]\n",
      " [101.75513]]\n",
      "540 Cost:  6.5510006 \n",
      "Prediction:\n",
      " [[154.80727]\n",
      " [182.11119]\n",
      " [181.39969]\n",
      " [197.79115]\n",
      " [137.83691]\n",
      " [101.7625 ]]\n",
      "550 Cost:  6.5279794 \n",
      "Prediction:\n",
      " [[154.79869]\n",
      " [182.11562]\n",
      " [181.39642]\n",
      " [197.78804]\n",
      " [137.84358]\n",
      " [101.76983]]\n",
      "560 Cost:  6.50511 \n",
      "Prediction:\n",
      " [[154.79016]\n",
      " [182.12006]\n",
      " [181.39319]\n",
      " [197.78497]\n",
      " [137.85023]\n",
      " [101.77713]]\n",
      "570 Cost:  6.4824066 \n",
      "Prediction:\n",
      " [[154.78168]\n",
      " [182.12447]\n",
      " [181.38997]\n",
      " [197.7819 ]\n",
      " [137.85687]\n",
      " [101.78442]]\n",
      "580 Cost:  6.4597993 \n",
      "Prediction:\n",
      " [[154.77318]\n",
      " [182.12885]\n",
      " [181.38673]\n",
      " [197.77881]\n",
      " [137.86348]\n",
      " [101.79168]]\n",
      "590 Cost:  6.437362 \n",
      "Prediction:\n",
      " [[154.76474 ]\n",
      " [182.13322 ]\n",
      " [181.38351 ]\n",
      " [197.77576 ]\n",
      " [137.87007 ]\n",
      " [101.798904]]\n",
      "600 Cost:  6.4150376 \n",
      "Prediction:\n",
      " [[154.75632 ]\n",
      " [182.1376  ]\n",
      " [181.38033 ]\n",
      " [197.7727  ]\n",
      " [137.87665 ]\n",
      " [101.806114]]\n",
      "610 Cost:  6.39286 \n",
      "Prediction:\n",
      " [[154.74794 ]\n",
      " [182.14195 ]\n",
      " [181.37715 ]\n",
      " [197.76967 ]\n",
      " [137.88322 ]\n",
      " [101.813286]]\n",
      "620 Cost:  6.370832 \n",
      "Prediction:\n",
      " [[154.73958 ]\n",
      " [182.14627 ]\n",
      " [181.37396 ]\n",
      " [197.76662 ]\n",
      " [137.88976 ]\n",
      " [101.820435]]\n",
      "630 Cost:  6.3489017 \n",
      "Prediction:\n",
      " [[154.73123]\n",
      " [182.1506 ]\n",
      " [181.37079]\n",
      " [197.76358]\n",
      " [137.89627]\n",
      " [101.82756]]\n",
      "640 Cost:  6.327137 \n",
      "Prediction:\n",
      " [[154.72293 ]\n",
      " [182.1549  ]\n",
      " [181.36763 ]\n",
      " [197.76056 ]\n",
      " [137.90277 ]\n",
      " [101.834656]]\n",
      "650 Cost:  6.3055267 \n",
      "Prediction:\n",
      " [[154.71465 ]\n",
      " [182.1592  ]\n",
      " [181.3645  ]\n",
      " [197.75755 ]\n",
      " [137.90924 ]\n",
      " [101.841736]]\n",
      "660 Cost:  6.2840075 \n",
      "Prediction:\n",
      " [[154.7064 ]\n",
      " [182.16348]\n",
      " [181.36136]\n",
      " [197.75455]\n",
      " [137.91571]\n",
      " [101.84878]]\n",
      "670 Cost:  6.262634 \n",
      "Prediction:\n",
      " [[154.69818]\n",
      " [182.16774]\n",
      " [181.35825]\n",
      " [197.75153]\n",
      " [137.92215]\n",
      " [101.8558 ]]\n",
      "680 Cost:  6.2413735 \n",
      "Prediction:\n",
      " [[154.68999]\n",
      " [182.17201]\n",
      " [181.35512]\n",
      " [197.74855]\n",
      " [137.92857]\n",
      " [101.86279]]\n",
      "690 Cost:  6.2202606 \n",
      "Prediction:\n",
      " [[154.68181]\n",
      " [182.17624]\n",
      " [181.35204]\n",
      " [197.74556]\n",
      " [137.93498]\n",
      " [101.86977]]\n",
      "700 Cost:  6.1992736 \n",
      "Prediction:\n",
      " [[154.67368]\n",
      " [182.18047]\n",
      " [181.34895]\n",
      " [197.74258]\n",
      " [137.94138]\n",
      " [101.87671]]\n",
      "710 Cost:  6.1783795 \n",
      "Prediction:\n",
      " [[154.66557]\n",
      " [182.18471]\n",
      " [181.34589]\n",
      " [197.73964]\n",
      " [137.94777]\n",
      " [101.88363]]\n",
      "720 Cost:  6.157661 \n",
      "Prediction:\n",
      " [[154.65749]\n",
      " [182.18889]\n",
      " [181.3428 ]\n",
      " [197.73665]\n",
      " [137.9541 ]\n",
      " [101.89052]]\n",
      "730 Cost:  6.137057 \n",
      "Prediction:\n",
      " [[154.64943]\n",
      " [182.19308]\n",
      " [181.33977]\n",
      " [197.73373]\n",
      " [137.96045]\n",
      " [101.89739]]\n",
      "740 Cost:  6.116562 \n",
      "Prediction:\n",
      " [[154.64139 ]\n",
      " [182.19725 ]\n",
      " [181.3367  ]\n",
      " [197.73076 ]\n",
      " [137.96675 ]\n",
      " [101.904236]]\n",
      "750 Cost:  6.096199 \n",
      "Prediction:\n",
      " [[154.63339]\n",
      " [182.20142]\n",
      " [181.33368]\n",
      " [197.72781]\n",
      " [137.97305]\n",
      " [101.91106]]\n",
      "760 Cost:  6.075962 \n",
      "Prediction:\n",
      " [[154.62541]\n",
      " [182.20555]\n",
      " [181.33064]\n",
      " [197.7249 ]\n",
      " [137.97934]\n",
      " [101.91785]]\n",
      "770 Cost:  6.0558314 \n",
      "Prediction:\n",
      " [[154.61745 ]\n",
      " [182.2097  ]\n",
      " [181.32765 ]\n",
      " [197.72197 ]\n",
      " [137.9856  ]\n",
      " [101.924614]]\n",
      "780 Cost:  6.0358377 \n",
      "Prediction:\n",
      " [[154.60953]\n",
      " [182.2138 ]\n",
      " [181.32465]\n",
      " [197.71906]\n",
      " [137.99185]\n",
      " [101.93136]]\n",
      "790 Cost:  6.0159535 \n",
      "Prediction:\n",
      " [[154.60162]\n",
      " [182.21793]\n",
      " [181.32166]\n",
      " [197.71616]\n",
      " [137.99808]\n",
      " [101.93808]]\n",
      "800 Cost:  5.9962044 \n",
      "Prediction:\n",
      " [[154.59375]\n",
      " [182.22202]\n",
      " [181.31866]\n",
      " [197.71324]\n",
      " [138.00427]\n",
      " [101.94476]]\n",
      "810 Cost:  5.976574 \n",
      "Prediction:\n",
      " [[154.5859 ]\n",
      " [182.22609]\n",
      " [181.31569]\n",
      " [197.71036]\n",
      " [138.01047]\n",
      " [101.95143]]\n",
      "820 Cost:  5.9570346 \n",
      "Prediction:\n",
      " [[154.57808]\n",
      " [182.23016]\n",
      " [181.31274]\n",
      " [197.70746]\n",
      " [138.01665]\n",
      " [101.95808]]\n",
      "830 Cost:  5.937651 \n",
      "Prediction:\n",
      " [[154.5703 ]\n",
      " [182.23422]\n",
      " [181.30978]\n",
      " [197.70459]\n",
      " [138.0228 ]\n",
      " [101.9647 ]]\n",
      "840 Cost:  5.918348 \n",
      "Prediction:\n",
      " [[154.56255 ]\n",
      " [182.2383  ]\n",
      " [181.30688 ]\n",
      " [197.70174 ]\n",
      " [138.02896 ]\n",
      " [101.971306]]\n",
      "850 Cost:  5.899215 \n",
      "Prediction:\n",
      " [[154.5548 ]\n",
      " [182.2423 ]\n",
      " [181.30392]\n",
      " [197.69885]\n",
      " [138.03505]\n",
      " [101.97787]]\n",
      "860 Cost:  5.880161 \n",
      "Prediction:\n",
      " [[154.54707]\n",
      " [182.24629]\n",
      " [181.30101]\n",
      " [197.696  ]\n",
      " [138.04117]\n",
      " [101.98442]]\n",
      "870 Cost:  5.861187 \n",
      "Prediction:\n",
      " [[154.53938]\n",
      " [182.25032]\n",
      " [181.2981 ]\n",
      " [197.69315]\n",
      " [138.04726]\n",
      " [101.99093]]\n",
      "880 Cost:  5.842386 \n",
      "Prediction:\n",
      " [[154.53172]\n",
      " [182.25432]\n",
      " [181.29521]\n",
      " [197.6903 ]\n",
      " [138.05331]\n",
      " [101.99744]]\n",
      "890 Cost:  5.823654 \n",
      "Prediction:\n",
      " [[154.5241 ]\n",
      " [182.25832]\n",
      " [181.29234]\n",
      " [197.68748]\n",
      " [138.05939]\n",
      " [102.00392]]\n",
      "900 Cost:  5.8050704 \n",
      "Prediction:\n",
      " [[154.51648]\n",
      " [182.26225]\n",
      " [181.28944]\n",
      " [197.68465]\n",
      " [138.06541]\n",
      " [102.01036]]\n",
      "910 Cost:  5.7865753 \n",
      "Prediction:\n",
      " [[154.50888]\n",
      " [182.26622]\n",
      " [181.28658]\n",
      " [197.68184]\n",
      " [138.07143]\n",
      " [102.01679]]\n",
      "920 Cost:  5.7682095 \n",
      "Prediction:\n",
      " [[154.50133]\n",
      " [182.27016]\n",
      " [181.28372]\n",
      " [197.67902]\n",
      " [138.07742]\n",
      " [102.02319]]\n",
      "930 Cost:  5.749952 \n",
      "Prediction:\n",
      " [[154.49379]\n",
      " [182.27408]\n",
      " [181.28088]\n",
      " [197.67621]\n",
      " [138.0834 ]\n",
      " [102.02957]]\n",
      "940 Cost:  5.731794 \n",
      "Prediction:\n",
      " [[154.48627]\n",
      " [182.27798]\n",
      " [181.27805]\n",
      " [197.6734 ]\n",
      " [138.08937]\n",
      " [102.03594]]\n",
      "950 Cost:  5.71373 \n",
      "Prediction:\n",
      " [[154.47879]\n",
      " [182.2819 ]\n",
      " [181.27522]\n",
      " [197.67061]\n",
      " [138.09532]\n",
      " [102.04226]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960 Cost:  5.695809 \n",
      "Prediction:\n",
      " [[154.47133]\n",
      " [182.28578]\n",
      " [181.27242]\n",
      " [197.66783]\n",
      " [138.10126]\n",
      " [102.04858]]\n",
      "970 Cost:  5.677986 \n",
      "Prediction:\n",
      " [[154.46388]\n",
      " [182.28966]\n",
      " [181.2696 ]\n",
      " [197.66505]\n",
      " [138.10716]\n",
      " [102.05487]]\n",
      "980 Cost:  5.6602383 \n",
      "Prediction:\n",
      " [[154.45648]\n",
      " [182.29353]\n",
      " [181.2668 ]\n",
      " [197.66228]\n",
      " [138.11307]\n",
      " [102.06112]]\n",
      "990 Cost:  5.642605 \n",
      "Prediction:\n",
      " [[154.4491 ]\n",
      " [182.2974 ]\n",
      " [181.264  ]\n",
      " [197.65952]\n",
      " [138.11896]\n",
      " [102.06737]]\n",
      "1000 Cost:  5.625111 \n",
      "Prediction:\n",
      " [[154.44174 ]\n",
      " [182.30124 ]\n",
      " [181.26125 ]\n",
      " [197.65677 ]\n",
      " [138.12482 ]\n",
      " [102.073586]]\n",
      "1010 Cost:  5.6076894 \n",
      "Prediction:\n",
      " [[154.4344 ]\n",
      " [182.30507]\n",
      " [181.25848]\n",
      " [197.65402]\n",
      " [138.13068]\n",
      " [102.07978]]\n",
      "1020 Cost:  5.590408 \n",
      "Prediction:\n",
      " [[154.4271 ]\n",
      " [182.30888]\n",
      " [181.25572]\n",
      " [197.65128]\n",
      " [138.13649]\n",
      " [102.08595]]\n",
      "1030 Cost:  5.573189 \n",
      "Prediction:\n",
      " [[154.41982]\n",
      " [182.3127 ]\n",
      " [181.25298]\n",
      " [197.64854]\n",
      " [138.14232]\n",
      " [102.0921 ]]\n",
      "1040 Cost:  5.5560856 \n",
      "Prediction:\n",
      " [[154.41254]\n",
      " [182.31648]\n",
      " [181.25024]\n",
      " [197.64581]\n",
      " [138.14812]\n",
      " [102.09823]]\n",
      "1050 Cost:  5.539081 \n",
      "Prediction:\n",
      " [[154.4053  ]\n",
      " [182.32028 ]\n",
      " [181.24753 ]\n",
      " [197.6431  ]\n",
      " [138.1539  ]\n",
      " [102.104324]]\n",
      "1060 Cost:  5.5221896 \n",
      "Prediction:\n",
      " [[154.39809]\n",
      " [182.32404]\n",
      " [181.2448 ]\n",
      " [197.64038]\n",
      " [138.15967]\n",
      " [102.11041]]\n",
      "1070 Cost:  5.505403 \n",
      "Prediction:\n",
      " [[154.3909 ]\n",
      " [182.3278 ]\n",
      " [181.2421 ]\n",
      " [197.63766]\n",
      " [138.1654 ]\n",
      " [102.11648]]\n",
      "1080 Cost:  5.4886994 \n",
      "Prediction:\n",
      " [[154.38374 ]\n",
      " [182.33156 ]\n",
      " [181.23943 ]\n",
      " [197.63498 ]\n",
      " [138.17116 ]\n",
      " [102.122505]]\n",
      "1090 Cost:  5.472109 \n",
      "Prediction:\n",
      " [[154.37659]\n",
      " [182.33527]\n",
      " [181.23671]\n",
      " [197.63228]\n",
      " [138.17686]\n",
      " [102.12851]]\n",
      "1100 Cost:  5.455601 \n",
      "Prediction:\n",
      " [[154.36948]\n",
      " [182.339  ]\n",
      " [181.23404]\n",
      " [197.6296 ]\n",
      " [138.18257]\n",
      " [102.13451]]\n",
      "1110 Cost:  5.4392114 \n",
      "Prediction:\n",
      " [[154.36238]\n",
      " [182.34271]\n",
      " [181.23137]\n",
      " [197.6269 ]\n",
      " [138.18825]\n",
      " [102.14048]]\n",
      "1120 Cost:  5.4228764 \n",
      "Prediction:\n",
      " [[154.35533]\n",
      " [182.34644]\n",
      " [181.22871]\n",
      " [197.62424]\n",
      " [138.19394]\n",
      " [102.14642]]\n",
      "1130 Cost:  5.4066873 \n",
      "Prediction:\n",
      " [[154.34827]\n",
      " [182.3501 ]\n",
      " [181.22607]\n",
      " [197.62157]\n",
      " [138.19958]\n",
      " [102.15234]]\n",
      "1140 Cost:  5.390557 \n",
      "Prediction:\n",
      " [[154.34125]\n",
      " [182.35378]\n",
      " [181.22342]\n",
      " [197.6189 ]\n",
      " [138.20523]\n",
      " [102.15825]]\n",
      "1150 Cost:  5.3745365 \n",
      "Prediction:\n",
      " [[154.33424]\n",
      " [182.35745]\n",
      " [181.2208 ]\n",
      " [197.61624]\n",
      " [138.21085]\n",
      " [102.16413]]\n",
      "1160 Cost:  5.358621 \n",
      "Prediction:\n",
      " [[154.32727]\n",
      " [182.3611 ]\n",
      " [181.21817]\n",
      " [197.61362]\n",
      " [138.21646]\n",
      " [102.16999]]\n",
      "1170 Cost:  5.3428154 \n",
      "Prediction:\n",
      " [[154.32033]\n",
      " [182.36473]\n",
      " [181.21556]\n",
      " [197.61098]\n",
      " [138.22205]\n",
      " [102.17582]]\n",
      "1180 Cost:  5.327063 \n",
      "Prediction:\n",
      " [[154.31339]\n",
      " [182.36836]\n",
      " [181.21295]\n",
      " [197.60832]\n",
      " [138.22762]\n",
      " [102.18164]]\n",
      "1190 Cost:  5.3114395 \n",
      "Prediction:\n",
      " [[154.30649]\n",
      " [182.37198]\n",
      " [181.21036]\n",
      " [197.6057 ]\n",
      " [138.23317]\n",
      " [102.18744]]\n",
      "1200 Cost:  5.2958636 \n",
      "Prediction:\n",
      " [[154.29959]\n",
      " [182.3756 ]\n",
      " [181.20778]\n",
      " [197.60309]\n",
      " [138.23872]\n",
      " [102.19321]]\n",
      "1210 Cost:  5.2804246 \n",
      "Prediction:\n",
      " [[154.29274]\n",
      " [182.3792 ]\n",
      " [181.20522]\n",
      " [197.60048]\n",
      " [138.24425]\n",
      " [102.19897]]\n",
      "1220 Cost:  5.2650514 \n",
      "Prediction:\n",
      " [[154.28592]\n",
      " [182.38278]\n",
      " [181.20265]\n",
      " [197.59787]\n",
      " [138.24977]\n",
      " [102.20469]]\n",
      "1230 Cost:  5.249789 \n",
      "Prediction:\n",
      " [[154.27908]\n",
      " [182.38634]\n",
      " [181.20007]\n",
      " [197.59528]\n",
      " [138.25525]\n",
      " [102.21039]]\n",
      "1240 Cost:  5.234614 \n",
      "Prediction:\n",
      " [[154.27231]\n",
      " [182.3899 ]\n",
      " [181.19754]\n",
      " [197.59268]\n",
      " [138.26074]\n",
      " [102.21608]]\n",
      "1250 Cost:  5.2195163 \n",
      "Prediction:\n",
      " [[154.26553]\n",
      " [182.39343]\n",
      " [181.19499]\n",
      " [197.59009]\n",
      " [138.2662 ]\n",
      " [102.22174]]\n",
      "1260 Cost:  5.204498 \n",
      "Prediction:\n",
      " [[154.25879]\n",
      " [182.39699]\n",
      " [181.19247]\n",
      " [197.5875 ]\n",
      " [138.27165]\n",
      " [102.22739]]\n",
      "1270 Cost:  5.189584 \n",
      "Prediction:\n",
      " [[154.25206]\n",
      " [182.40051]\n",
      " [181.18994]\n",
      " [197.58493]\n",
      " [138.27708]\n",
      " [102.23302]]\n",
      "1280 Cost:  5.174742 \n",
      "Prediction:\n",
      " [[154.24536]\n",
      " [182.40402]\n",
      " [181.18744]\n",
      " [197.58235]\n",
      " [138.28252]\n",
      " [102.23862]]\n",
      "1290 Cost:  5.1599956 \n",
      "Prediction:\n",
      " [[154.2387  ]\n",
      " [182.40755 ]\n",
      " [181.18495 ]\n",
      " [197.57979 ]\n",
      " [138.28792 ]\n",
      " [102.244194]]\n",
      "1300 Cost:  5.1453357 \n",
      "Prediction:\n",
      " [[154.23204]\n",
      " [182.41103]\n",
      " [181.18245]\n",
      " [197.57724]\n",
      " [138.29332]\n",
      " [102.24975]]\n",
      "1310 Cost:  5.1307454 \n",
      "Prediction:\n",
      " [[154.22539 ]\n",
      " [182.4145  ]\n",
      " [181.17995 ]\n",
      " [197.57468 ]\n",
      " [138.29869 ]\n",
      " [102.255295]]\n",
      "1320 Cost:  5.1162934 \n",
      "Prediction:\n",
      " [[154.2188 ]\n",
      " [182.41798]\n",
      " [181.17749]\n",
      " [197.57216]\n",
      " [138.30405]\n",
      " [102.26082]]\n",
      "1330 Cost:  5.101855 \n",
      "Prediction:\n",
      " [[154.2122 ]\n",
      " [182.42145]\n",
      " [181.17502]\n",
      " [197.5696 ]\n",
      " [138.3094 ]\n",
      " [102.26631]]\n",
      "1340 Cost:  5.0875745 \n",
      "Prediction:\n",
      " [[154.20564]\n",
      " [182.4249 ]\n",
      " [181.17256]\n",
      " [197.56708]\n",
      " [138.31471]\n",
      " [102.27179]]\n",
      "1350 Cost:  5.0733366 \n",
      "Prediction:\n",
      " [[154.1991  ]\n",
      " [182.42833 ]\n",
      " [181.17012 ]\n",
      " [197.56456 ]\n",
      " [138.32004 ]\n",
      " [102.277245]]\n",
      "1360 Cost:  5.059182 \n",
      "Prediction:\n",
      " [[154.19257 ]\n",
      " [182.43175 ]\n",
      " [181.16766 ]\n",
      " [197.56203 ]\n",
      " [138.32533 ]\n",
      " [102.282684]]\n",
      "1370 Cost:  5.0450974 \n",
      "Prediction:\n",
      " [[154.18608]\n",
      " [182.4352 ]\n",
      " [181.16524]\n",
      " [197.55952]\n",
      " [138.33063]\n",
      " [102.28811]]\n",
      "1380 Cost:  5.031136 \n",
      "Prediction:\n",
      " [[154.1796 ]\n",
      " [182.43858]\n",
      " [181.16283]\n",
      " [197.55702]\n",
      " [138.33589]\n",
      " [102.2935 ]]\n",
      "1390 Cost:  5.0172143 \n",
      "Prediction:\n",
      " [[154.17314 ]\n",
      " [182.44199 ]\n",
      " [181.16042 ]\n",
      " [197.55452 ]\n",
      " [138.34116 ]\n",
      " [102.298874]]\n",
      "1400 Cost:  5.003374 \n",
      "Prediction:\n",
      " [[154.1667 ]\n",
      " [182.44537]\n",
      " [181.158  ]\n",
      " [197.55203]\n",
      " [138.3464 ]\n",
      " [102.30422]]\n",
      "1410 Cost:  4.9896283 \n",
      "Prediction:\n",
      " [[154.1603 ]\n",
      " [182.44876]\n",
      " [181.15561]\n",
      " [197.54953]\n",
      " [138.35162]\n",
      " [102.30956]]\n",
      "1420 Cost:  4.9759674 \n",
      "Prediction:\n",
      " [[154.1539 ]\n",
      " [182.45212]\n",
      " [181.15323]\n",
      " [197.54706]\n",
      " [138.35684]\n",
      " [102.31487]]\n",
      "1430 Cost:  4.96238 \n",
      "Prediction:\n",
      " [[154.14754]\n",
      " [182.45547]\n",
      " [181.15085]\n",
      " [197.54459]\n",
      " [138.36205]\n",
      " [102.32017]]\n",
      "1440 Cost:  4.9488697 \n",
      "Prediction:\n",
      " [[154.14117]\n",
      " [182.4588 ]\n",
      " [181.14847]\n",
      " [197.54211]\n",
      " [138.36723]\n",
      " [102.32545]]\n",
      "1450 Cost:  4.935453 \n",
      "Prediction:\n",
      " [[154.13484]\n",
      " [182.46213]\n",
      " [181.1461 ]\n",
      " [197.53964]\n",
      " [138.37239]\n",
      " [102.3307 ]]\n",
      "1460 Cost:  4.922114 \n",
      "Prediction:\n",
      " [[154.12854]\n",
      " [182.46545]\n",
      " [181.14377]\n",
      " [197.5372 ]\n",
      " [138.37755]\n",
      " [102.33594]]\n",
      "1470 Cost:  4.9088225 \n",
      "Prediction:\n",
      " [[154.12225 ]\n",
      " [182.46878 ]\n",
      " [181.14142 ]\n",
      " [197.53474 ]\n",
      " [138.38269 ]\n",
      " [102.341156]]\n",
      "1480 Cost:  4.8956475 \n",
      "Prediction:\n",
      " [[154.116  ]\n",
      " [182.47208]\n",
      " [181.1391 ]\n",
      " [197.5323 ]\n",
      " [138.38782]\n",
      " [102.34636]]\n",
      "1490 Cost:  4.8825154 \n",
      "Prediction:\n",
      " [[154.10976]\n",
      " [182.47537]\n",
      " [181.13675]\n",
      " [197.52986]\n",
      " [138.39293]\n",
      " [102.35153]]\n",
      "1500 Cost:  4.8694615 \n",
      "Prediction:\n",
      " [[154.10353]\n",
      " [182.47864]\n",
      " [181.13443]\n",
      " [197.52742]\n",
      " [138.39804]\n",
      " [102.35669]]\n",
      "1510 Cost:  4.8565288 \n",
      "Prediction:\n",
      " [[154.09734]\n",
      " [182.4819 ]\n",
      " [181.13213]\n",
      " [197.525  ]\n",
      " [138.4031 ]\n",
      " [102.36183]]\n",
      "1520 Cost:  4.8436227 \n",
      "Prediction:\n",
      " [[154.09116]\n",
      " [182.48517]\n",
      " [181.12982]\n",
      " [197.52258]\n",
      " [138.40819]\n",
      " [102.36695]]\n",
      "1530 Cost:  4.8308015 \n",
      "Prediction:\n",
      " [[154.08499]\n",
      " [182.48842]\n",
      " [181.12753]\n",
      " [197.52016]\n",
      " [138.41324]\n",
      " [102.37205]]\n",
      "1540 Cost:  4.818047 \n",
      "Prediction:\n",
      " [[154.07886]\n",
      " [182.49167]\n",
      " [181.12524]\n",
      " [197.51776]\n",
      " [138.41829]\n",
      " [102.37713]]\n",
      "1550 Cost:  4.805373 \n",
      "Prediction:\n",
      " [[154.07272]\n",
      " [182.49489]\n",
      " [181.12296]\n",
      " [197.51535]\n",
      " [138.42331]\n",
      " [102.38219]]\n",
      "1560 Cost:  4.7927823 \n",
      "Prediction:\n",
      " [[154.06664]\n",
      " [182.49811]\n",
      " [181.1207 ]\n",
      " [197.51295]\n",
      " [138.42833]\n",
      " [102.38724]]\n",
      "1570 Cost:  4.7802396 \n",
      "Prediction:\n",
      " [[154.06056]\n",
      " [182.50133]\n",
      " [181.11844]\n",
      " [197.51056]\n",
      " [138.43333]\n",
      " [102.39225]]\n",
      "1580 Cost:  4.767791 \n",
      "Prediction:\n",
      " [[154.0545  ]\n",
      " [182.50452 ]\n",
      " [181.11618 ]\n",
      " [197.50818 ]\n",
      " [138.43832 ]\n",
      " [102.397255]]\n",
      "1590 Cost:  4.7553988 \n",
      "Prediction:\n",
      " [[154.04848]\n",
      " [182.50772]\n",
      " [181.11394]\n",
      " [197.5058 ]\n",
      " [138.4433 ]\n",
      " [102.40224]]\n",
      "1600 Cost:  4.743079 \n",
      "Prediction:\n",
      " [[154.04245]\n",
      " [182.5109 ]\n",
      " [181.1117 ]\n",
      " [197.50342]\n",
      " [138.44826]\n",
      " [102.4072 ]]\n",
      "1610 Cost:  4.7308536 \n",
      "Prediction:\n",
      " [[154.03647]\n",
      " [182.51407]\n",
      " [181.10947]\n",
      " [197.50107]\n",
      " [138.4532 ]\n",
      " [102.41215]]\n",
      "1620 Cost:  4.718679 \n",
      "Prediction:\n",
      " [[154.03049]\n",
      " [182.51723]\n",
      " [181.10724]\n",
      " [197.49869]\n",
      " [138.45813]\n",
      " [102.41709]]\n",
      "1630 Cost:  4.7065864 \n",
      "Prediction:\n",
      " [[154.02452 ]\n",
      " [182.52037 ]\n",
      " [181.10503 ]\n",
      " [197.49634 ]\n",
      " [138.46304 ]\n",
      " [102.422005]]\n",
      "1640 Cost:  4.694554 \n",
      "Prediction:\n",
      " [[154.0186  ]\n",
      " [182.52351 ]\n",
      " [181.10283 ]\n",
      " [197.49399 ]\n",
      " [138.46796 ]\n",
      " [102.426895]]\n",
      "1650 Cost:  4.6825995 \n",
      "Prediction:\n",
      " [[154.01268]\n",
      " [182.52663]\n",
      " [181.10063]\n",
      " [197.49165]\n",
      " [138.47285]\n",
      " [102.43177]]\n",
      "1660 Cost:  4.6707034 \n",
      "Prediction:\n",
      " [[154.00677 ]\n",
      " [182.52974 ]\n",
      " [181.09843 ]\n",
      " [197.4893  ]\n",
      " [138.47772 ]\n",
      " [102.436615]]\n",
      "1670 Cost:  4.6588717 \n",
      "Prediction:\n",
      " [[154.0009 ]\n",
      " [182.53287]\n",
      " [181.09627]\n",
      " [197.48698]\n",
      " [138.48259]\n",
      " [102.44145]]\n",
      "1680 Cost:  4.6470947 \n",
      "Prediction:\n",
      " [[153.99504 ]\n",
      " [182.53596 ]\n",
      " [181.09409 ]\n",
      " [197.48463 ]\n",
      " [138.48744 ]\n",
      " [102.446266]]\n",
      "1690 Cost:  4.6353974 \n",
      "Prediction:\n",
      " [[153.98921 ]\n",
      " [182.53905 ]\n",
      " [181.09192 ]\n",
      " [197.48232 ]\n",
      " [138.4923  ]\n",
      " [102.451065]]\n",
      "1700 Cost:  4.6237817 \n",
      "Prediction:\n",
      " [[153.9834 ]\n",
      " [182.54213]\n",
      " [181.08977]\n",
      " [197.48   ]\n",
      " [138.49712]\n",
      " [102.45586]]\n",
      "1710 Cost:  4.6122327 \n",
      "Prediction:\n",
      " [[153.9776 ]\n",
      " [182.5452 ]\n",
      " [181.0876 ]\n",
      " [197.47768]\n",
      " [138.5019 ]\n",
      " [102.46061]]\n",
      "1720 Cost:  4.600729 \n",
      "Prediction:\n",
      " [[153.97183 ]\n",
      " [182.54826 ]\n",
      " [181.08548 ]\n",
      " [197.47539 ]\n",
      " [138.50673 ]\n",
      " [102.465355]]\n",
      "1730 Cost:  4.5893335 \n",
      "Prediction:\n",
      " [[153.96606 ]\n",
      " [182.5513  ]\n",
      " [181.08334 ]\n",
      " [197.47308 ]\n",
      " [138.51149 ]\n",
      " [102.470085]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740 Cost:  4.577948 \n",
      "Prediction:\n",
      " [[153.96033]\n",
      " [182.55437]\n",
      " [181.0812 ]\n",
      " [197.4708 ]\n",
      " [138.51627]\n",
      " [102.4748 ]]\n",
      "1750 Cost:  4.566629 \n",
      "Prediction:\n",
      " [[153.9546 ]\n",
      " [182.5574 ]\n",
      " [181.0791 ]\n",
      " [197.4685 ]\n",
      " [138.52104]\n",
      " [102.47949]]\n",
      "1760 Cost:  4.555389 \n",
      "Prediction:\n",
      " [[153.9489 ]\n",
      " [182.56042]\n",
      " [181.077  ]\n",
      " [197.46622]\n",
      " [138.52579]\n",
      " [102.48416]]\n",
      "1770 Cost:  4.544235 \n",
      "Prediction:\n",
      " [[153.94322 ]\n",
      " [182.56342 ]\n",
      " [181.07489 ]\n",
      " [197.46393 ]\n",
      " [138.53052 ]\n",
      " [102.488815]]\n",
      "1780 Cost:  4.533123 \n",
      "Prediction:\n",
      " [[153.93756 ]\n",
      " [182.56644 ]\n",
      " [181.07278 ]\n",
      " [197.46167 ]\n",
      " [138.53523 ]\n",
      " [102.493454]]\n",
      "1790 Cost:  4.522052 \n",
      "Prediction:\n",
      " [[153.93192]\n",
      " [182.56943]\n",
      " [181.0707 ]\n",
      " [197.45938]\n",
      " [138.53995]\n",
      " [102.49806]]\n",
      "1800 Cost:  4.5110717 \n",
      "Prediction:\n",
      " [[153.9263 ]\n",
      " [182.57242]\n",
      " [181.06863]\n",
      " [197.45712]\n",
      " [138.54465]\n",
      " [102.50266]]\n",
      "1810 Cost:  4.500176 \n",
      "Prediction:\n",
      " [[153.9207 ]\n",
      " [182.5754 ]\n",
      " [181.06654]\n",
      " [197.45488]\n",
      " [138.54932]\n",
      " [102.50726]]\n",
      "1820 Cost:  4.489302 \n",
      "Prediction:\n",
      " [[153.9151  ]\n",
      " [182.57835 ]\n",
      " [181.06447 ]\n",
      " [197.45262 ]\n",
      " [138.55399 ]\n",
      " [102.511826]]\n",
      "1830 Cost:  4.478519 \n",
      "Prediction:\n",
      " [[153.90953]\n",
      " [182.5813 ]\n",
      " [181.06242]\n",
      " [197.45038]\n",
      " [138.55864]\n",
      " [102.51637]]\n",
      "1840 Cost:  4.4677553 \n",
      "Prediction:\n",
      " [[153.90398 ]\n",
      " [182.58426 ]\n",
      " [181.06038 ]\n",
      " [197.44814 ]\n",
      " [138.5633  ]\n",
      " [102.520905]]\n",
      "1850 Cost:  4.4570723 \n",
      "Prediction:\n",
      " [[153.89845]\n",
      " [182.58719]\n",
      " [181.05833]\n",
      " [197.44589]\n",
      " [138.56793]\n",
      " [102.52541]]\n",
      "1860 Cost:  4.4464545 \n",
      "Prediction:\n",
      " [[153.89294 ]\n",
      " [182.59012 ]\n",
      " [181.05627 ]\n",
      " [197.44365 ]\n",
      " [138.57254 ]\n",
      " [102.529915]]\n",
      "1870 Cost:  4.435911 \n",
      "Prediction:\n",
      " [[153.88745]\n",
      " [182.59303]\n",
      " [181.05426]\n",
      " [197.44144]\n",
      " [138.57715]\n",
      " [102.5344 ]]\n",
      "1880 Cost:  4.4254055 \n",
      "Prediction:\n",
      " [[153.88197 ]\n",
      " [182.59595 ]\n",
      " [181.05225 ]\n",
      " [197.43921 ]\n",
      " [138.58174 ]\n",
      " [102.538864]]\n",
      "1890 Cost:  4.414967 \n",
      "Prediction:\n",
      " [[153.87653]\n",
      " [182.59885]\n",
      " [181.05023]\n",
      " [197.43701]\n",
      " [138.58633]\n",
      " [102.54331]]\n",
      "1900 Cost:  4.4046082 \n",
      "Prediction:\n",
      " [[153.8711 ]\n",
      " [182.60175]\n",
      " [181.04822]\n",
      " [197.43481]\n",
      " [138.59088]\n",
      " [102.54774]]\n",
      "1910 Cost:  4.394232 \n",
      "Prediction:\n",
      " [[153.86566]\n",
      " [182.60464]\n",
      " [181.04623]\n",
      " [197.43259]\n",
      " [138.59546]\n",
      " [102.55215]]\n",
      "1920 Cost:  4.384008 \n",
      "Prediction:\n",
      " [[153.86026]\n",
      " [182.6075 ]\n",
      " [181.04424]\n",
      " [197.4304 ]\n",
      " [138.59999]\n",
      " [102.55656]]\n",
      "1930 Cost:  4.3737884 \n",
      "Prediction:\n",
      " [[153.85487]\n",
      " [182.61037]\n",
      " [181.04225]\n",
      " [197.4282 ]\n",
      " [138.60452]\n",
      " [102.56093]]\n",
      "1940 Cost:  4.363616 \n",
      "Prediction:\n",
      " [[153.84952 ]\n",
      " [182.61324 ]\n",
      " [181.04027 ]\n",
      " [197.42603 ]\n",
      " [138.60905 ]\n",
      " [102.565285]]\n",
      "1950 Cost:  4.3535366 \n",
      "Prediction:\n",
      " [[153.84416]\n",
      " [182.61607]\n",
      " [181.03831]\n",
      " [197.42384]\n",
      " [138.61356]\n",
      " [102.56963]]\n",
      "1960 Cost:  4.3434787 \n",
      "Prediction:\n",
      " [[153.83884]\n",
      " [182.61893]\n",
      " [181.03635]\n",
      " [197.42166]\n",
      " [138.61806]\n",
      " [102.57397]]\n",
      "1970 Cost:  4.333517 \n",
      "Prediction:\n",
      " [[153.83353]\n",
      " [182.62175]\n",
      " [181.0344 ]\n",
      " [197.4195 ]\n",
      " [138.62253]\n",
      " [102.57828]]\n",
      "1980 Cost:  4.323557 \n",
      "Prediction:\n",
      " [[153.82823 ]\n",
      " [182.62457 ]\n",
      " [181.03244 ]\n",
      " [197.41731 ]\n",
      " [138.62701 ]\n",
      " [102.582565]]\n",
      "1990 Cost:  4.3136992 \n",
      "Prediction:\n",
      " [[153.82297 ]\n",
      " [182.62738 ]\n",
      " [181.03049 ]\n",
      " [197.41516 ]\n",
      " [138.63147 ]\n",
      " [102.586845]]\n",
      "2000 Cost:  4.3038907 \n",
      "Prediction:\n",
      " [[153.8177 ]\n",
      " [182.63017]\n",
      " [181.02855]\n",
      " [197.41301]\n",
      " [138.63591]\n",
      " [102.5911 ]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Set up feed_dict variables inside the loop.\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score will be  [[177.40349]]\n",
      "Other score will be  [[168.29636]\n",
      " [180.70924]]\n"
     ]
    }
   ],
   "source": [
    "# Ask mys score\n",
    "print(\"Your score will be \", sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))\n",
    "print(\"Other score will be \", sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## logistic regression\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1670614\n",
      "200 0.5899467\n",
      "400 0.56663394\n",
      "600 0.54477936\n",
      "800 0.5242119\n",
      "1000 0.50479513\n",
      "1200 0.4864204\n",
      "1400 0.4690012\n",
      "1600 0.45246863\n",
      "1800 0.43676603\n",
      "2000 0.42184567\n",
      "2200 0.40766537\n",
      "2400 0.3941871\n",
      "2600 0.38137496\n",
      "2800 0.36919555\n",
      "3000 0.35761634\n",
      "3200 0.34660628\n",
      "3400 0.3361354\n",
      "3600 0.32617483\n",
      "3800 0.31669685\n",
      "4000 0.30767488\n",
      "4200 0.29908368\n",
      "4400 0.29089895\n",
      "4600 0.2830979\n",
      "4800 0.27565855\n",
      "5000 0.26856053\n",
      "5200 0.26178434\n",
      "5400 0.2553118\n",
      "5600 0.24912553\n",
      "5800 0.24320935\n",
      "6000 0.23754792\n",
      "6200 0.2321272\n",
      "6400 0.22693352\n",
      "6600 0.22195445\n",
      "6800 0.21717829\n",
      "7000 0.21259373\n",
      "7200 0.20819068\n",
      "7400 0.20395929\n",
      "7600 0.19989051\n",
      "7800 0.1959758\n",
      "8000 0.19220711\n",
      "8200 0.18857694\n",
      "8400 0.18507825\n",
      "8600 0.18170448\n",
      "8800 0.17844945\n",
      "9000 0.17530717\n",
      "9200 0.17227226\n",
      "9400 0.16933958\n",
      "9600 0.16650419\n",
      "9800 0.16376159\n",
      "10000 0.16110739\n",
      "\n",
      "Hypothesis:  [[0.03577117]\n",
      " [0.16514964]\n",
      " [0.32810417]\n",
      " [0.7709588 ]\n",
      " [0.93281764]\n",
      " [0.9779285 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 6 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lab 6-1 : Softmax Classifier\n",
    "x_data = [[1, 2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3]) # 보통 y의 갯수가 lable의 갯수가 됨.\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight') # x의 값이 4개, y의 값이 3개 \n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias') # b는 출력값과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.5917115\n",
      "200 0.5575094\n",
      "400 0.4588519\n",
      "600 0.38367784\n",
      "800 0.31319326\n",
      "1000 0.24404404\n",
      "1200 0.21768582\n",
      "1400 0.19856958\n",
      "1600 0.18243949\n",
      "1800 0.16865301\n",
      "2000 0.15674049\n",
      "[[7.2602569e-03 9.9272823e-01 1.1476767e-05]] [1]\n",
      "[[7.2602569e-03 9.9272823e-01 1.1476767e-05]\n",
      " [8.2788390e-01 1.4773960e-01 2.4376478e-02]\n",
      " [1.5645517e-08 3.6790612e-04 9.9963212e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "            \n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1))) # arg_max함수를 통해 one-hot encoding을 해줌.\n",
    "    \n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1,11,7,9],[1,3,4,3],[1,1,0,1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_one_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-d27a978a00e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Cross entropy cost/loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcost_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_one_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_one_hot' is not defined"
     ]
    }
   ],
   "source": [
    "# ### Lab 6-2 : Fancy SoftmaxClassifier\n",
    "# ### cross_entropy, one_hot, reshape\n",
    "\n",
    "# ## softmax_corss_entropy_with_logits\n",
    "\n",
    "# logits = tf.matmul(X, W) + b\n",
    "# hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# # Cross entropy cost/loss\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) # Y는 one-hot으로 주어짐\n",
    "\n",
    "# # Cross entropy cost/loss\n",
    "# cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
    "# cost = tf.reduce_mean(cost_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Animal classification with softmax_cross_entropy_with_logits\n",
    "# Predictin animal type based on various features\n",
    "xy = np.loadtxt('data/data-04-zoo.csv', delimiter=\",\", dtype=np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "nb_classes = 7 # 0 ~ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1]) # 0 ~ 6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes) # one hot\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 5.882\tAcc: 39.60%\n",
      "Step:   100\tLoss: 0.884\tAcc: 74.26%\n",
      "Step:   200\tLoss: 0.545\tAcc: 84.16%\n",
      "Step:   300\tLoss: 0.406\tAcc: 88.12%\n",
      "Step:   400\tLoss: 0.326\tAcc: 90.10%\n",
      "Step:   500\tLoss: 0.272\tAcc: 91.09%\n",
      "Step:   600\tLoss: 0.233\tAcc: 93.07%\n",
      "Step:   700\tLoss: 0.203\tAcc: 93.07%\n",
      "Step:   800\tLoss: 0.178\tAcc: 94.06%\n",
      "Step:   900\tLoss: 0.159\tAcc: 94.06%\n",
      "Step:  1000\tLoss: 0.142\tAcc: 95.05%\n",
      "Step:  1100\tLoss: 0.128\tAcc: 96.04%\n",
      "Step:  1200\tLoss: 0.117\tAcc: 98.02%\n",
      "Step:  1300\tLoss: 0.107\tAcc: 98.02%\n",
      "Step:  1400\tLoss: 0.098\tAcc: 98.02%\n",
      "Step:  1500\tLoss: 0.091\tAcc: 99.01%\n",
      "Step:  1600\tLoss: 0.084\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.078\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.074\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.069\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                X: x_data, Y: y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-1427b42e1f55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's see if we can predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# y_data: (N,1) = flatten => (N, ) matches pred.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[{}] Prediction: {} True Y: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\helloTF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\helloTF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helloTF",
   "language": "python",
   "name": "hellotf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
